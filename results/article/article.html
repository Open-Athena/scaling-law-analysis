<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Hidden Pitfalls of Chinchilla Approach 2</title>
    <meta name="description"
        content="An analysis of systematic biases in scaling law inference using the IsoFLOP parabolic fitting method.">

    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap"
        rel="stylesheet">

    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f9fafb;
            --bg-tertiary: #f3f4f6;
            --text-primary: #191919;
            --text-secondary: #525252;
            --text-muted: #8b8b8b;
            --accent-primary: #c4704f;
            --accent-secondary: #d4a574;
            --accent-success: #16a34a;
            --accent-error: #dc2626;
            --border-color: #e5e5e5;
            --code-bg: #f5f5f5;
            --code-text: #c4704f;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.75;
            font-size: 17px;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        .container {
            max-width: 920px;
            margin: 0 auto;
            padding: 5rem 2rem;
        }

        /* Header */
        header {
            margin-bottom: 3.5rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--border-color);
        }

        h1 {
            font-size: 2.75rem;
            font-weight: 600;
            line-height: 1.15;
            margin-bottom: 1.25rem;
            color: var(--text-primary);
            letter-spacing: -0.02em;
        }

        .subtitle {
            font-size: 1.25rem;
            color: var(--text-secondary);
            font-weight: 400;
            line-height: 1.5;
        }

        /* Sections */
        section {
            margin-bottom: 4rem;
        }

        h2 {
            font-size: 1.75rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: var(--text-primary);
            padding-top: 1rem;
            letter-spacing: -0.01em;
        }

        h3 {
            font-size: 1.25rem;
            font-weight: 600;
            margin: 2.5rem 0 1rem;
            color: var(--text-primary);
        }

        h4 {
            font-size: 1.1rem;
            font-weight: 600;
            margin: 2rem 0 1rem;
            color: var(--text-primary);
        }

        p {
            margin-bottom: 1.5rem;
            color: var(--text-primary);
        }

        a {
            color: var(--accent-primary);
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s ease;
        }

        a:hover {
            border-bottom-color: var(--accent-primary);
        }

        /* Lists */
        ul,
        ol {
            margin-bottom: 1.5rem;
            padding-left: 1.5rem;
        }

        li {
            margin-bottom: 0.6rem;
            color: var(--text-primary);
        }

        /* Emphasis */
        strong {
            font-weight: 600;
            color: var(--text-primary);
        }

        em {
            font-style: italic;
        }

        /* Code */
        code {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.875em;
            background: var(--code-bg);
            padding: 0.2em 0.45em;
            border-radius: 4px;
            color: var(--code-text);
        }

        /* Math blocks */
        .math-block {
            background: var(--bg-secondary);
            border-left: 3px solid var(--accent-secondary);
            padding: 1.5rem 1.75rem;
            margin: 2rem 0;
            border-radius: 0 6px 6px 0;
            overflow-x: auto;
        }

        /* Figures */
        figure {
            margin: 2.5rem 0;
            text-align: center;
        }

        figure img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08), 0 4px 12px rgba(0, 0, 0, 0.05);
        }

        figcaption {
            margin-top: 1rem;
            font-size: 0.9rem;
            color: var(--text-secondary);
            text-align: left;
            line-height: 1.6;
        }

        figcaption strong {
            color: var(--text-primary);
        }

        /* Tables */
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            font-size: 0.95rem;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        .comparison-table th {
            background: var(--bg-secondary);
            font-weight: 600;
            color: var(--text-primary);
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.03em;
        }

        .comparison-table td {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.875em;
        }

        .comparison-table tbody tr {
            transition: background 0.15s ease;
        }

        .comparison-table tbody tr:hover {
            background: var(--bg-tertiary);
        }

        /* Error highlighting */
        .error-negligible {
            color: var(--accent-success);
            font-weight: 500;
        }

        .error-significant {
            color: var(--accent-error);
            font-weight: 600;
            background: rgba(220, 38, 38, 0.08);
            padding: 0.15em 0.35em;
            border-radius: 3px;
        }

        /* Callouts */
        .callout {
            background: var(--bg-secondary);
            border-radius: 8px;
            padding: 1.5rem 1.75rem;
            margin: 2rem 0;
            border: 1px solid var(--border-color);
        }

        .callout.key-point {
            border-left: 4px solid var(--accent-primary);
            border-left-width: 4px;
        }

        .callout.warning {
            border-left: 4px solid #d97706;
        }

        .callout-title {
            font-weight: 600;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--text-primary);
            font-size: 0.9rem;
        }

        .callout p {
            color: var(--text-secondary);
            font-size: 0.95rem;
        }

        .callout p:last-child {
            margin-bottom: 0;
        }

        /* Process steps */
        .process-steps {
            background: var(--bg-secondary);
            border-radius: 8px;
            padding: 1.5rem 1.5rem 1.5rem 2rem;
            margin: 2rem 0;
            border: 1px solid var(--border-color);
        }

        .process-steps ol {
            margin: 0;
            padding-left: 1.2rem;
        }

        .process-steps li {
            padding: 0.65rem 0;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        .process-steps li strong {
            color: var(--text-primary);
        }

        .process-steps li:last-child {
            border-bottom: none;
        }

        /* Responsive */
        @media (max-width: 600px) {
            .container {
                padding: 2.5rem 1.25rem;
            }

            h1 {
                font-size: 2rem;
            }

            h2 {
                font-size: 1.5rem;
            }

            .comparison-table {
                font-size: 0.85rem;
            }

            .comparison-table th,
            .comparison-table td {
                padding: 0.75rem 0.5rem;
            }

            .callout,
            .process-steps,
            .math-block {
                padding: 1.25rem;
            }
        }
    </style>
</head>

<body>
    <article class="container">
        <header>
            <h1>The Hidden Pitfalls of Chinchilla Approach 2</h1>
            <p class="subtitle">Systematic biases in scaling law inference from IsoFLOP parabolic fitting</p>
        </header>

        <!-- Section 1: Approaches to Fitting Scaling Laws -->
        <section id="approaches">
            <h2>Approaches to Fitting Scaling Laws</h2>

            <p>
                Neural scaling laws describe how model performance improves with compute.
                The Chinchilla loss surface models this relationship as:
            </p>

            <div class="math-block">
                \[
                L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}
                \]
            </div>

            <p>
                where \(N\) is the number of parameters, \(D\) is the number of training tokens,
                \(E\) is the irreducible loss, and \(A, B, \alpha, \beta\) capture how quickly
                performance improves with scale.
            </p>

            <p>
                Given a compute budget \(C \approx 6ND\), the optimal allocation satisfies:
            </p>

            <div class="math-block">
                \[
                N^* \propto C^a \quad \text{where} \quad a = \frac{\beta}{\alpha + \beta}
                \]
                \[
                D^* \propto C^b \quad \text{where} \quad b = \frac{\alpha}{\alpha + \beta}
                \]
            </div>

            <p>
                Recovering the exponents \(a\) and \(b\) from empirical training runs is crucial
                for planning efficient large-scale training. Two canonical approaches exist:
            </p>

            <h3>Approach 2: IsoFLOP Parabolic Fitting</h3>

            <p>
                This is the workhorse method from the Chinchilla paper. The key insight is that
                along a fixed-compute contour (IsoFLOP curve), loss as a function of \(\log N\)
                is approximately parabolic near the optimum.
            </p>

            <div class="process-steps">
                <ol>
                    <li>
                        <strong>Sample IsoFLOP contours:</strong> For each compute budget \(C\),
                        train models at various \((N, D)\) pairs satisfying \(C = 6ND\)
                    </li>
                    <li>
                        <strong>Fit parabolas:</strong> For each budget, fit
                        \(L = a(\log N)^2 + b(\log N) + c\) and extract the minimum \(N^*\)
                    </li>
                    <li>
                        <strong>Fit power laws:</strong> Regress \(\log N^*\) against \(\log C\)
                        to recover the exponent \(a\) (and similarly for \(D^*\), \(b\))
                    </li>
                </ol>
            </div>

            <p>
                The appeal is simplicity: only polynomial fits, no nonlinear optimization.
                The parabolic approximation comes from a Taylor expansion of the loss surface
                around the optimum.
            </p>

            <h3>Approach 3: Direct Surface Fitting</h3>

            <p>
                The alternative is to fit all five parameters \((E, A, B, \alpha, \beta)\)
                simultaneously via nonlinear least squares. This avoids the parabolic
                approximation entirely but is notoriously unstable‚Äîhighly sensitive to
                initialization and prone to converging to spurious local minima.
            </p>

            <div class="callout key-point">
                <div class="callout-title">üìå Article Focus</div>
                <p>
                    This article examines pitfalls of <strong>Approach 2</strong> using
                    noise-free synthetic data. By eliminating statistical noise, we isolate
                    the systematic biases inherent to the method itself.
                </p>
            </div>
        </section>

        <!-- Section 2: The Happy Path -->
        <section id="happy-path">
            <h2>The Happy Path: Symmetric Surfaces</h2>

            <p>
                Before examining failure modes, let's establish that Approach 2 works
                perfectly under ideal conditions. Consider a <strong>symmetric</strong>
                loss surface where \(\alpha = \beta\):
            </p>

            <div class="math-block">
                \[
                L(N, D) = 1.69 + \frac{400}{N^{0.31}} + \frac{400}{D^{0.31}}
                \]
            </div>

            <p>
                With equal exponents, the optimal allocation splits compute evenly between
                parameters and data. The true scaling exponents are:
            </p>

            <div class="math-block">
                \[
                a = b = \frac{0.31}{0.31 + 0.31} = 0.5
                \]
            </div>

            <p>
                We sample five IsoFLOP contours spanning \(10^{17}\) to \(10^{21}\) FLOPs,
                fit parabolas to each, and extract the optimal token count \(D^*\).
            </p>

            <figure>
                <img src="happy_path.png" alt="Approach 2 on symmetric surface showing perfect recovery">
                <figcaption>
                    <strong>Figure 1:</strong> Approach 2 applied to a symmetric loss surface.
                    Left: IsoFLOP curves with fitted parabolas. True (√ó) and inferred (+) optima
                    are indistinguishable. Right: Power-law fit recovers the exact scaling exponent.
                </figcaption>
            </figure>

            <p>
                The results confirm perfect recovery of the token scaling exponent and intercept:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>True Value</th>
                        <th>Inferred Value</th>
                        <th>Relative Error</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>b (D* exponent)</td>
                        <td>0.500000</td>
                        <td>0.500000</td>
                        <td>+6.2√ó10‚Åª¬π¬≤%</td>
                    </tr>
                    <tr>
                        <td>b‚ÇÄ (D* intercept)</td>
                        <td>‚àí0.389076</td>
                        <td>‚àí0.389076</td>
                        <td>‚àí1.4√ó10‚Åª¬π‚Å∞%</td>
                    </tr>
                </tbody>
            </table>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    On a symmetric loss surface with perfect sampling, Approach 2 recovers
                    both exponents and intercepts with machine-precision accuracy. The
                    parabolic approximation is exact when \(\alpha = \beta\).
                </p>
            </div>

            <p>
                This establishes our baseline: Approach 2 is not inherently flawed. The
                problems arise when we deviate from these ideal conditions‚Äîas we'll see
                in the following sections.
            </p>
        </section>

        <!-- Section 3: Asymmetric Surfaces -->
        <section id="asymmetric">
            <h2>Asymmetric Surfaces: When Intercepts Go Wrong</h2>

            <p>
                We repeat the exact same procedure as before‚Äîperfect sampling centers, no noise,
                identical methodology. The only change: the loss surface is now <strong>asymmetric</strong>
                (\(\alpha \neq \beta\)).
            </p>

            <h3>What Happens</h3>

            <p>
                Simulation results show that when the loss surface is asymmetric, Approach 2 produces
                systematically wrong intercepts while exponents remain accurate. This isn't statistical
                noise‚Äîit's a deterministic bias from fitting parabolas to a non-parabolic surface.
            </p>

            <p>
                We test two configurations to see how the effect scales:
            </p>

            <ul>
                <li><strong>Chinchilla:</strong> \(\alpha = 0.34\), \(\beta = 0.28\) (ratio ‚âà 1.2)</li>
                <li><strong>High Imbalance:</strong> \(\alpha = 0.46\), \(\beta = 0.15\) (ratio = 3.0)</li>
            </ul>

            <figure>
                <img src="asymmetric.png" alt="Approach 2 on asymmetric surfaces showing intercept errors">
                <figcaption>
                    <strong>Figure 2:</strong> Approach 2 on asymmetric loss surfaces.
                    Note the visible gap between true (dashed) and inferred (solid) power-law
                    lines in the High Imbalance case‚Äîthe exponents match perfectly, but
                    the intercepts differ.
                </figcaption>
            </figure>

            <h4>Chinchilla Surface</h4>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>True Value</th>
                        <th>Inferred Value</th>
                        <th>Relative Error</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>b (D* exponent)</td>
                        <td>0.548387</td>
                        <td>0.548387</td>
                        <td class="error-negligible">‚âà 0%</td>
                    </tr>
                    <tr>
                        <td>b‚ÇÄ (D* intercept)</td>
                        <td>‚àí0.555357</td>
                        <td>‚àí0.577937</td>
                        <td class="error-significant">‚àí4.1%</td>
                    </tr>
                </tbody>
            </table>

            <h4>High Imbalance Surface</h4>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>True Value</th>
                        <th>Inferred Value</th>
                        <th>Relative Error</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>b (D* exponent)</td>
                        <td>0.750000</td>
                        <td>0.750000</td>
                        <td class="error-negligible">‚âà 0%</td>
                    </tr>
                    <tr>
                        <td>b‚ÇÄ (D* intercept)</td>
                        <td>‚àí1.345791</td>
                        <td>‚àí1.459200</td>
                        <td class="error-significant">‚àí8.4%</td>
                    </tr>
                </tbody>
            </table>

            <h3>Why This Is Surprising</h3>

            <p>
                A few percent error in the intercept might seem minor, but consider that this simulation
                gave Approach 2 every advantage. The data is perfect‚Äîno measurement noise, with every
                point lying exactly on the true loss surface. The sampling is perfect too, with IsoFLOP
                grids centered precisely at the true optimum‚Äîsomething you wouldn't know how to do in
                practice. And the parameters are standard, taken directly from the Chinchilla paper
                rather than constructed to expose a weakness.
            </p>

            <div class="callout key-point">
                <div class="callout-title">üìå Key Finding</div>
                <p>
                    Even under these ideal conditions, Approach 2 produces biased intercepts for
                    asymmetric surfaces. The error is systematic‚Äîa property of the parabolic
                    approximation, not statistical noise.
                </p>
            </div>

            <h3>Why It Happens</h3>

            <p>
                The IsoFLOP loss curve is not a true parabola‚Äîit contains exponential terms.
                When a parabola is fit to this curve, the parabola's minimum (vertex) doesn't
                land exactly at the true optimum. It shifts slightly‚Äîand the key insight is that
                this shift depends only on the loss surface shape (\(\alpha\), \(\beta\)) and
                the sampling grid. It does not depend on compute budget. The sampling grid size
                becomes important here: wider grids amplify the mismatch between the true curve
                and its parabolic approximation, increasing the vertex shift.
            </p>

            <p>
                Since the vertex shift is constant across all compute budgets, it biases every
                inferred \(N^*\) by the same multiplicative factor. When fitting
                \(\log N^*\) vs \(\log C\) to extract scaling exponents:
            </p>

            <ul>
                <li>The <strong>slope (exponent)</strong> is unchanged‚Äîmultiplying all \(N^*\) values
                    by a constant factor adds a constant to \(\log N^*\), which doesn't affect the slope</li>
                <li>The <strong>intercept</strong> absorbs the entire error‚Äîit's biased by exactly
                    that multiplicative factor</li>
            </ul>

            <p>
                <strong>Exact derivation:</strong> The intercept error can be derived analytically
                in closed form. The parabola vertex shifts by \(\delta w\) (in log-space), giving
                an intercept error of:
            </p>

            <div class="math-block">
                \[
                \text{Intercept error} = 10^{\delta w} - 1
                \]
            </div>

            <p>
                where \(\delta w = f(\alpha, \beta, W, n)\) depends only on the surface exponents
                and the sampling grid (width \(W\), number of points \(n\))‚Äînot on \(C\), \(E\),
                \(A\), or \(B\). Key properties:
            </p>

            <ul>
                <li>\(\delta w = 0\) when \(\alpha = \beta\) (symmetric surfaces have no error)</li>
                <li>\(\delta w\) grows with \(|\alpha - \beta|\) (more asymmetry ‚Üí more error)</li>
                <li>\(\delta w\) grows with \(W\) (wider sampling range ‚Üí more error)</li>
            </ul>

            <p>
                For example, with the Chinchilla parameters (\(\alpha = 0.34\), \(\beta = 0.28\)):
                a narrow grid (\(W = 0.3\)) yields 0.3% intercept error, while a wider grid
                (\(W = 2.0\)) yields 15.5% error. <a href="scaling_exponent_errors.html">See the
                full derivation</a> for the closed-form vertex shift formula.
            </p>

            <p>
                <strong>Intuition via Taylor expansion:</strong> A parabola is a 2nd-order polynomial,
                which is equivalent to a 2nd-order Taylor expansion around the optimum. The approximation
                \(L(w) \approx L(0) + \frac{1}{2}L''(0)w^2\) is only valid when higher-order terms are
                negligible‚Äîi.e., when samples are close to the true minimum. As sampling range increases,
                3rd and 4th order terms grow. For symmetric surfaces (\(\alpha = \beta\)), odd-order
                terms cancel by symmetry, preserving the vertex location. For asymmetric surfaces,
                they don't cancel, shifting the fitted vertex away from the true optimum.
            </p>

            <h3>Why It Matters</h3>

            <!-- TODO: Fill in extrapolation analysis -->
            <p>
                <em>[WIP: Extrapolation analysis showing how intercept errors compound at higher compute budgets.
                Will include: D* = b‚ÇÄ √ó C^b formula, error quantification at 10¬≤¬≤ to 10¬≤‚Åµ FLOPs,
                figure showing extrapolation error vs compute budget.]</em>
            </p>

        </section>

    </article>
</body>

</html>