<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Problems with Chinchilla Approach 2</title>
    <meta name="description"
        content="Systematic biases in scaling law inference from IsoFLOP parabola fits.">

    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap"
        rel="stylesheet">

    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f9fafb;
            --bg-tertiary: #f3f4f6;
            --text-primary: #191919;
            --text-secondary: #525252;
            --text-muted: #8b8b8b;
            --accent-primary: #c4704f;
            --accent-secondary: #d4a574;
            --accent-success: #16a34a;
            --accent-error: #dc2626;
            --border-color: #e5e5e5;
            --code-bg: #f5f5f5;
            --code-text: #c4704f;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.75;
            font-size: 17px;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        .container {
            max-width: 920px;
            margin: 0 auto;
            padding: 3rem 2rem;
        }

        /* Header */
        header {
            margin-bottom: 1.5rem;
            padding-bottom: 1.5rem;
            border-bottom: 1px solid var(--border-color);
        }

        h1 {
            font-size: 2.75rem;
            font-weight: 600;
            line-height: 1.15;
            margin-bottom: 1.25rem;
            color: var(--text-primary);
            letter-spacing: -0.02em;
        }

        .subtitle {
            font-size: 1.25rem;
            color: var(--text-secondary);
            font-weight: 400;
            line-height: 1.5;
        }

        /* Hero image */
        .hero-image {
            margin: 0 0 2rem;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.12), 0 8px 24px rgba(0, 0, 0, 0.08);
        }

        .hero-image img {
            width: 100%;
            display: block;
        }

        /* Table of Contents */
        nav.toc {
            margin-bottom: 2.5rem;
            padding: 1.75rem 2rem;
            background: var(--bg-secondary);
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        nav.toc h2 {
            font-size: 1rem;
            font-weight: 600;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: var(--text-muted);
            margin-bottom: 1rem;
            padding-top: 0;
        }

        nav.toc ol {
            list-style: none;
            padding-left: 0;
            margin-bottom: 0;
            counter-reset: toc-counter;
        }

        nav.toc > ol > li {
            counter-increment: toc-counter;
            margin-bottom: 0.4rem;
        }

        nav.toc > ol > li > a::before {
            content: counter(toc-counter) ". ";
            color: var(--text-muted);
        }

        nav.toc a {
            color: var(--text-primary);
            font-size: 0.95rem;
            font-weight: 500;
            border-bottom: none;
        }

        nav.toc a:hover {
            color: var(--accent-primary);
            border-bottom: none;
        }

        nav.toc ol ol {
            padding-left: 1.5rem;
            margin-top: 0.3rem;
            margin-bottom: 0.5rem;
        }

        nav.toc ol ol li {
            margin-bottom: 0.2rem;
        }

        nav.toc ol ol a {
            font-weight: 400;
            font-size: 0.9rem;
            color: var(--text-secondary);
        }

        /* Sections */
        section {
            margin-bottom: 4rem;
        }

        h2 {
            font-size: 1.75rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: var(--text-primary);
            padding-top: 1rem;
            letter-spacing: -0.01em;
        }

        h3 {
            font-size: 1.25rem;
            font-weight: 600;
            margin: 2.5rem 0 1rem;
            color: var(--text-primary);
        }

        h4 {
            font-size: 1.1rem;
            font-weight: 600;
            margin: 2rem 0 1rem;
            color: var(--text-primary);
        }

        p {
            margin-bottom: 1.5rem;
            color: var(--text-primary);
        }

        a {
            color: var(--accent-primary);
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s ease;
        }

        a:hover {
            border-bottom-color: var(--accent-primary);
        }

        /* Lists */
        ul,
        ol {
            margin-bottom: 1.5rem;
            padding-left: 1.5rem;
        }

        li {
            margin-bottom: 0.6rem;
            color: var(--text-primary);
        }

        /* Emphasis */
        strong {
            font-weight: 600;
            color: var(--text-primary);
        }

        em {
            font-style: italic;
        }

        /* Code */
        code {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.875em;
            background: var(--code-bg);
            padding: 0.2em 0.45em;
            border-radius: 4px;
            color: var(--code-text);
        }

        /* Math blocks */
        .math-block {
            background: var(--bg-secondary);
            border-left: 3px solid var(--accent-secondary);
            padding: 1.5rem 1.75rem;
            margin: 2rem 0;
            border-radius: 0 6px 6px 0;
            overflow-x: auto;
        }

        /* Figures */
        figure {
            margin: 2.5rem 0;
            text-align: center;
        }

        figure img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08), 0 4px 12px rgba(0, 0, 0, 0.05);
        }

        figcaption {
            margin-top: 1rem;
            font-size: 0.9rem;
            color: var(--text-secondary);
            text-align: left;
            line-height: 1.6;
        }

        figcaption strong {
            color: var(--text-primary);
        }

        /* Tables */
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            font-size: 0.95rem;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        .comparison-table th {
            background: var(--bg-secondary);
            font-weight: 600;
            color: var(--text-primary);
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.03em;
        }

        .comparison-table td {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.875em;
        }

        .comparison-table tbody tr {
            transition: background 0.15s ease;
        }

        .comparison-table tbody tr:hover {
            background: var(--bg-tertiary);
        }

        /* Collapsible data tables */
        details.data-table-container {
            margin: 1.5rem 0;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            background: var(--bg-secondary);
        }

        details.data-table-container summary {
            padding: 0.75rem 1.25rem;
            cursor: pointer;
            font-weight: 500;
            color: var(--text-secondary);
            font-size: 0.9rem;
            user-select: none;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        details.data-table-container summary:hover {
            background: var(--bg-tertiary);
            border-radius: 8px;
        }

        details.data-table-container[open] summary {
            border-bottom: 1px solid var(--border-color);
            border-radius: 8px 8px 0 0;
        }

        details.data-table-container .data-table-wrapper {
            padding: 1rem;
            overflow-x: auto;
        }

        .data-table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.85rem;
        }

        .data-table th,
        .data-table td {
            padding: 0.6rem 0.75rem;
            text-align: right;
            border-bottom: 1px solid var(--border-color);
            white-space: nowrap;
        }

        .data-table th {
            background: var(--bg-tertiary);
            font-weight: 600;
            color: var(--text-primary);
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.03em;
        }

        .data-table td {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8em;
        }

        .data-table td:first-child,
        .data-table th:first-child {
            text-align: left;
        }

        .data-table tbody tr:hover {
            background: rgba(196, 112, 79, 0.05);
        }

        .data-table td[title] {
            cursor: help;
            border-bottom: 1px dotted var(--text-muted);
        }

        .data-table .error-zero {
            color: var(--accent-success);
            font-weight: 500;
        }

        .data-table .error-low {
            color: #ca8a04;
            font-weight: 600;
        }

        .data-table .error-medium {
            color: #ea580c;
            font-weight: 600;
        }

        .data-table .error-high {
            color: var(--accent-error);
            font-weight: 700;
            background: rgba(220, 38, 38, 0.1);
            border-radius: 3px;
            padding: 0.15em 0.35em;
        }

        .data-table .surface-header {
            font-weight: 600;
            background: var(--bg-tertiary);
            color: var(--text-primary);
        }

        .data-table .surface-header td {
            font-family: 'Inter', sans-serif;
            font-size: 0.85rem;
        }

        /* Error highlighting */
        .error-negligible {
            color: var(--accent-success);
            font-weight: 500;
        }

        .error-significant {
            color: var(--accent-error);
            font-weight: 600;
            background: rgba(220, 38, 38, 0.08);
            padding: 0.15em 0.35em;
            border-radius: 3px;
        }

        /* Callouts */
        .callout {
            background: var(--bg-secondary);
            border-radius: 8px;
            padding: 1.5rem 1.75rem;
            margin: 2rem 0;
            border: 1px solid var(--border-color);
        }

        .callout.key-point {
            border-left: 4px solid var(--accent-primary);
            border-left-width: 4px;
        }

        .callout.warning {
            border-left: 4px solid #d97706;
        }

        .callout-title {
            font-weight: 600;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--text-primary);
            font-size: 0.9rem;
        }

        .callout p {
            color: var(--text-secondary);
            font-size: 0.95rem;
        }

        .callout p:last-child {
            margin-bottom: 0;
        }

        /* Process steps */
        .process-steps {
            background: var(--bg-secondary);
            border-radius: 8px;
            padding: 1.5rem 1.5rem 1.5rem 2rem;
            margin: 2rem 0;
            border: 1px solid var(--border-color);
        }

        .process-steps ol {
            margin: 0;
            padding-left: 1.2rem;
        }

        .process-steps li {
            padding: 0.65rem 0;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        .process-steps li strong {
            color: var(--text-primary);
        }

        .process-steps li:last-child {
            border-bottom: none;
        }

        /* Responsive */
        @media (max-width: 600px) {
            .container {
                padding: 2.5rem 1.25rem;
            }

            h1 {
                font-size: 2rem;
            }

            h2 {
                font-size: 1.5rem;
            }

            .comparison-table {
                font-size: 0.85rem;
            }

            .comparison-table th,
            .comparison-table td {
                padding: 0.75rem 0.5rem;
            }

            .callout,
            .process-steps,
            .math-block {
                padding: 1.25rem;
            }
        }
    </style>
</head>

<body>
    <article class="container">
        <header>
            <h1>Problems with Chinchilla Approach 2</h1>
            <p class="subtitle">Systematic biases in scaling law inference from IsoFLOP parabola fits</p>
        </header>

        <!-- Hero Image -->
        <div class="hero-image">
            <img src="static/isoflop_hero.png" alt="Artistic rendering of IsoFLOP curves">
        </div>

        <!-- Table of Contents -->
        <nav class="toc">
            <h2>Contents</h2>
            <ol>
                <li><a href="#motivation">Motivation</a></li>
                <li>
                    <a href="#approaches">Preliminaries: Loss Surface, Notation, and Fitting Methods</a>
                    <ol>
                        <li><a href="#approach-2">Approach 2: IsoFLOP Parabolic Fitting</a></li>
                        <li><a href="#approach-3">Approach 3: Direct Surface Fitting</a></li>
                    </ol>
                </li>
                <li><a href="#happy-path">The Happy Path: Symmetric Surfaces</a></li>
                <li>
                    <a href="#asymmetric">Asymmetric Surfaces: Intercept and Extrapolation Errors</a>
                    <ol>
                        <li><a href="#what-happens">What Happens</a></li>
                        <li><a href="#why-surprising">Why This Is Surprising</a></li>
                        <li><a href="#why-it-happens">Why It Happens</a></li>
                        <li><a href="#why-it-matters">Why It Matters</a></li>
                    </ol>
                </li>
                <li>
                    <a href="#off-center">Off-Center Sampling: Exponent and Extrapolation Errors</a>
                    <ol>
                        <li><a href="#constant-bias">Constant Multiplicative Bias</a></li>
                        <li><a href="#drifting-bias">Drifting Bias</a></li>
                    </ol>
                </li>
                <li>
                    <a href="#in-the-wild">IsoFLOP Curves in the Wild: Evidence from Published Studies</a>
                    <ol>
                        <li><a href="#putting-it-together">Compounding Errors</a></li>
                    </ol>
                </li>
                <li>
                    <a href="#robust-fits">Robust Fits: Unbiased Estimation with Linear Separation</a>
                    <ol>
                        <li><a href="#approach-3-problems">Problems with Direct Surface Fitting</a></li>
                        <li><a href="#vpnls">Variable Projection (VPNLS)</a></li>
                        <li><a href="#method-comparison-recovery">Method Comparison (Parameter Recovery)</a></li>
                        <li><a href="#method-comparison-inference">Method Comparison (Exponent Inference)</a></li>
                    </ol>
                </li>
                <li>
                    <a href="#conclusion">Conclusion</a>
                    <ol>
                        <li><a href="#limitations">Limitations</a></li>
                    </ol>
                </li>
                <li>
                    <a href="#appendix">Appendix</a>
                    <ol>
                        <li><a href="#appendix-method-comparison">Detailed Method Comparison</a></li>
                        <li><a href="#appendix-combined-extrapolation">Combined Extrapolation Error by Compute Budget</a></li>
                        <li><a href="#appendix-noisy-isoflop">IsoFLOP Samples with Noise</a></li>
                        <li><a href="#appendix-exponent-errors">Exponent Inference Error Breakdown</a></li>
                    </ol>
                </li>
            </ol>
        </nav>

        <!-- Section: Motivation -->
        <section id="motivation">
            <h2>Motivation</h2>

            <p>
                Chinchilla Approach 2 is arguably the most widely adopted method for fitting
                scaling laws in practice today. Introduced in the original Chinchilla
                paper<sup><a href="#ref-chinchilla">[1]</a></sup>, it has since been used by leading AI labs including
                DeepMind<sup><a href="#ref-chinchilla">[1]</a>,<a href="#ref-sovit">[7]</a></sup>
                (its creators),
                Meta<sup><a href="#ref-llama3">[2]</a>,<a href="#ref-optibert">[9]</a></sup>,
                DeepSeek<sup><a href="#ref-deepseek">[3]</a></sup>,
                Microsoft<sup><a href="#ref-ehr_scaling">[4]</a></sup>,
                Amazon<sup><a href="#ref-il_scaling">[6]</a></sup>,
                Waymo<sup><a href="#ref-waymo_scaling">[8]</a></sup>, and
                Arc Institute<sup><a href="#ref-evo">[5]</a></sup>, among others. It is also
                a workhorse method for academic scaling law
                studies<sup><a href="#ref-dit_scaling">[10]</a>,<a href="#ref-dlm_scaling">[11]</a>,<a href="#ref-biosignal_scaling">[12]</a></sup>
                and high-profile practitioner <a href="https://github.com/karpathy/nanochat/discussions/420">tutorials</a> from researchers like Andrej Karpathy.
            </p>

            <p>
                The method's appeal lies in its stability and data efficiency relative to
                nonlinear optimization over all loss surface parameters. Rather than fitting
                all five parameters of the loss surface simultaneously, Approach 2 targets
                only the two scaling exponents, relying on second-order Taylor approximations
                that reduce each IsoFLOP curve to a simple parabola. This sacrifices recovery
                of the full loss surface but makes estimation far more stable and
                data-efficient, letting practitioners extract the most actionable quantities
                for compute allocation planning through a sequence of straightforward
                polynomial and linear fits, without ever touching a nonlinear optimizer.
            </p>

            <p>
                Despite this broad adoption, the sensitivity of the method's core
                approximations and its behavior on loss surfaces that are less symmetric than
                the original Chinchilla form (where parameter and token scaling exponents are
                roughly equal) have not, to our knowledge, been studied in detail. Here we
                revisit the basics of how to apply a simple model like Chinchilla with high
                precision and stability, to validation loss alone, before considering more
                advanced extensions. We investigate through noise-free synthetic simulations
                that isolate systematic biases inherent to the method itself by eliminating
                all sources of statistical noise.
            </p>

            <p>
                We show how these biases affect downstream decisions like dataset size
                selection for final training runs at large compute budgets. We show how
                extrapolation errors trace back to suboptimal IsoFLOP experiment design, and
                that pathologies in these designs can be observed in real, high-profile
                scaling law studies even if they are difficult to quantify precisely. Finally,
                we propose an alternative fitting method that is simple, stable, and free of
                these biases while building on the same intuitive computational shortcut:
                optimizing exponential terms separately from linear terms. We call this
                approach Variable Projection with Non-negative Least Squares (VPNLS).
            </p>

            <p>
                This investigation is also motivated by a broader landscape of
                <strong><em>analytical</em></strong> extensions to the Chinchilla loss
                surface. A growing body of work adds or modifies terms in the original
                functional form to account for additional training configuration choices
                such as data
                repetition<sup><a href="#ref-data_constrained">[17]</a>,<a href="#ref-data_filtering_scaling">[14]</a></sup>,
                overfitting<sup><a href="#ref-mupt">[18]</a></sup>,
                precision<sup><a href="#ref-precision_scaling">[19]</a></sup>,
                MoE sparsity<sup><a href="#ref-moe_scaling">[24]</a></sup>,
                data quality<sup><a href="#ref-quality_scaling">[21]</a></sup>,
                data mixtures<sup><a href="#ref-optimal_data_mixtures">[22]</a>,<a href="#ref-redundancy_scaling">[23]</a>,<a href="#ref-data_filtering_scaling">[14]</a></sup>,
                non-embedding parameters<sup><a href="#ref-reconciling_scaling">[20]</a></sup>,
                and downstream task
                performance<sup><a href="#ref-ai2_task_scaling">[25]</a></sup>,
                to name a few. These extensions prescribe explicit functional forms rather
                than inferring scaling law structure automatically, and they build directly
                on the Chinchilla model as a foundation. A fitting method that recovers the
                base surface with higher precision may therefore offer a stronger starting
                point for these richer settings as well.
            </p>
        </section>

        <!-- Section 1: Approaches to Fitting Scaling Laws -->
        <section id="approaches">
            <h2>Preliminaries: Loss Surface, Notation, and Fitting Methods</h2>

            <p>
                Neural scaling laws describe how model performance improves with compute.
                The Chinchilla loss surface models this relationship as:
            </p>

            <div class="math-block">
                \[
                L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}
                \]
            </div>

            <p>
                where \(N\) is the number of parameters, \(D\) is the number of training tokens,
                \(E\) is the irreducible loss, and \(A, B, \alpha, \beta\) capture how quickly
                performance improves with scale.
            </p>

            <p>
                Given a compute budget \(C \approx 6ND\), the optimal allocation satisfies:
            </p>

            <div class="math-block">
                \[
                N^* \propto C^a \quad \text{where} \quad a = \frac{\beta}{\alpha + \beta}
                \]
                \[
                D^* \propto C^b \quad \text{where} \quad b = \frac{\alpha}{\alpha + \beta}
                \]
            </div>

            <p>
                Recovering the exponents \(a\) and \(b\) from empirical training runs is crucial
                for planning efficient large-scale training. Two canonical approaches exist:
            </p>

            <h3 id="approach-2">Approach 2: IsoFLOP Parabolic Fitting</h3>

            <p>
                This method is presented in the Chinchilla paper. The key insight is that
                along a fixed-compute contour (IsoFLOP curve), loss as a function of \(\log N\)
                is approximately parabolic near the optimum.
            </p>

            <div class="process-steps">
                <ol>
                    <li>
                        <strong>Sample IsoFLOP contours:</strong> For each compute budget \(C\),
                        train models at various \((N, D)\) pairs satisfying \(C = 6ND\)
                    </li>
                    <li>
                        <strong>Fit parabolas:</strong> For each budget, fit
                        \(L = p(\log N)^2 + q(\log N) + r\) and extract the minimum \(N^*\)
                    </li>
                    <li>
                        <strong>Fit power laws:</strong> Regress \(\log N^*\) against \(\log C\)
                        to recover the exponent \(a\) (and similarly for \(D^*\), \(b\))
                    </li>
                </ol>
            </div>

            <p>
                The appeal is simplicity: only polynomial fits, no nonlinear optimization.
                The parabolic approximation comes from a Taylor expansion of the loss surface
                around the optimum.
            </p>

            <h3 id="approach-3">Approach 3: Direct Surface Fitting</h3>

            <p>
                The alternative is to fit all five parameters \((E, A, B, \alpha, \beta)\)
                simultaneously via nonlinear least squares. This avoids the parabolic
                approximation entirely but is notoriously unstable: highly sensitive to
                initialization and prone to converging to spurious local minima.
            </p>
        </section>

        <!-- Section 2: The Happy Path -->
        <section id="happy-path">
            <h2>The Happy Path: Symmetric Surfaces</h2>

            <p>
                Before examining failure modes, let's establish that Approach 2 works
                perfectly under ideal conditions. Consider a <strong>symmetric</strong>
                loss surface where \(\alpha = \beta\):
            </p>

            <div class="math-block">
                \[
                L(N, D) = 1.69 + \frac{400}{N^{0.31}} + \frac{400}{D^{0.31}}
                \]
            </div>

            <p>
                With equal exponents, the optimal allocation splits compute evenly between
                parameters and data. The true scaling exponents are:
            </p>

            <div class="math-block">
                \[
                a = b = \frac{0.31}{0.31 + 0.31} = 0.5
                \]
            </div>

            <p>
                We sample five IsoFLOP contours spanning \(10^{17}\) to \(10^{21}\) FLOPs,
                with 15 model sizes per curve, fit parabolas to each, and extract
                the optimal token count \(D^*\). All simulations throughout this
                article use these same five compute budgets and 15 points per
                IsoFLOP curve.
            </p>

            <figure>
                <img src="figures/happy_path/happy_path.png" alt="Approach 2 on symmetric surface showing perfect recovery">
                <figcaption>
                    <strong>Figure 1:</strong> Approach 2 applied to a symmetric loss surface.
                    Left: IsoFLOP curves with fitted parabolas. True (√ó) and inferred (+) optima
                    are indistinguishable. Right: Power-law fit recovers the exact scaling exponent.
                </figcaption>
            </figure>

            <p>
                The results confirm perfect recovery of the token scaling exponent and intercept:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>True Value</th>
                        <th>Inferred Value</th>
                        <th>Relative Error</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>b (D* exponent)</td>
                        <td>0.500000</td>
                        <td>0.500000</td>
                        <td>+6.2√ó10‚Åª¬π¬≤%</td>
                    </tr>
                    <tr>
                        <td>b‚ÇÄ (D* intercept)</td>
                        <td>‚àí0.389076</td>
                        <td>‚àí0.389076</td>
                        <td>‚àí1.4√ó10‚Åª¬π‚Å∞%</td>
                    </tr>
                </tbody>
            </table>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    On a symmetric loss surface with perfectly crafted IsoFLOP grid sampling, 
                    Approach 2 recovers both exponents and intercepts with machine-precision 
                    accuracy. When \(\alpha = \beta\), the parabola vertex shift is zero,
                    so the inferred optima coincide with the true optima.
                </p>
            </div>

            <p>
                This establishes our baseline. Approach 2 is precisely correct under ideal conditions that 
                are unrealistic in practice. The problems arise when we deviate from these ideal conditions, as we'll see
                in the following sections where these conditions are perturbed in controlled ways.
            </p>
        </section>

        <!-- Section 3: Asymmetric Surfaces -->
        <section id="asymmetric">
            <h2>Asymmetric Surfaces: Intercept and Extrapolation Errors</h2>

            <p>
                We repeat the exact same procedure as before: perfect sampling centers, no noise,
                identical methodology. The only change is that the loss surface is now <strong>asymmetric</strong>
                (\(\alpha \neq \beta\)).
            </p>

            <h3 id="what-happens">What Happens</h3>

            <p>
                Simulation results show that when the loss surface is asymmetric, Approach 2 produces
                systematically wrong intercepts while exponents remain accurate. This isn't statistical
                noise; it's a deterministic bias from fitting parabolas to a non-parabolic surface.
            </p>

            <p>
                We test two configurations to see how the effect scales:
            </p>

            <ul>
                <li><strong>Chinchilla:</strong> \(\alpha = 0.34\), \(\beta = 0.28\) (ratio ‚âà 1.2)</li>
                <li><strong>Asymmetric:</strong> \(\alpha = 0.46\), \(\beta = 0.15\) (ratio = 3.0)</li>
            </ul>

            <p>
                The Asymmetric surface is not a contrived stress test. An exponent ratio of 3.0 is
                comparable to what has been observed in practice, e.g. DeepSeek<sup><a href="#ref-deepseek">[3]</a></sup>
                reports compute-optimal allocation exponents of \(a = 0.73\), \(b = 0.27\) for
                an OpenWebText2 variant. This implies a loss surface exponent ratio of
                \(\beta / \alpha \approx 2.7\). The asymmetry runs in the opposite direction from our
                Asymmetric surface (\(\beta > \alpha\) rather than \(\alpha > \beta\)), but the degree
                of imbalance is similar, and it is the magnitude of the imbalance, not its direction,
                that drives the biases studied here.
            </p>

            <figure>
                <img src="figures/asymmetric/asymmetric.png" alt="Approach 2 on asymmetric surfaces showing intercept errors">
                <figcaption>
                    <strong>Figure 2:</strong> Approach 2 on asymmetric loss surfaces.
                    Note the visible gap between true (dashed) and inferred (solid) power-law
                    lines in the Asymmetric case. The exponents match perfectly, but
                    the intercepts differ.
                </figcaption>
            </figure>

            <h4>Chinchilla Surface</h4>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>True Value</th>
                        <th>Inferred Value</th>
                        <th>Relative Error</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>b (D* exponent)</td>
                        <td>0.548387</td>
                        <td>0.548387</td>
                        <td class="error-negligible">‚âà 0%</td>
                    </tr>
                    <tr>
                        <td>b‚ÇÄ (D* intercept)</td>
                        <td>‚àí0.555357</td>
                        <td>‚àí0.578092</td>
                        <td class="error-significant">‚àí4.1%</td>
                    </tr>
                </tbody>
            </table>

            <h4>Asymmetric Surface</h4>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>True Value</th>
                        <th>Inferred Value</th>
                        <th>Relative Error</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>b (D* exponent)</td>
                        <td>0.750000</td>
                        <td>0.750000</td>
                        <td class="error-negligible">‚âà 0%</td>
                    </tr>
                    <tr>
                        <td>b‚ÇÄ (D* intercept)</td>
                        <td>‚àí1.345791</td>
                        <td>‚àí1.459957</td>
                        <td class="error-significant">‚àí8.5%</td>
                    </tr>
                </tbody>
            </table>

            <h3 id="why-surprising">Why This Is Surprising</h3>

            <p>
                A few percent error in the intercept might seem minor, but consider that this simulation
                gave Approach 2 every advantage. The data is perfect: no measurement noise, with every
                point lying exactly on the true loss surface. The sampling is perfect too, with IsoFLOP
                grids centered precisely at the true optimum (something you wouldn't know how to do in
                practice). And the parameters are standard, taken directly from the Chinchilla paper
                rather than contrived to expose a potentially unrealistic weakness.
            </p>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    Even under these ideal conditions, Approach 2 produces biased intercepts for
                    asymmetric surfaces. The error is systematic, a property of the parabolic
                    approximation, not statistical noise.
                </p>
            </div>

            <h3 id="why-it-happens">Why It Happens</h3>

            <p>
                The IsoFLOP loss curve is not a true parabola; it contains exponential terms.
                When a parabola is fit to this curve, the parabola's minimum (vertex) doesn't
                land exactly at the true optimum. It shifts slightly, and the key insight is that
                this shift depends only on the loss surface shape (\(\alpha\), \(\beta\)) and
                the sampling grid. It does not depend on compute budget. The sampling grid size
                becomes important here: wider grids amplify the mismatch between the true curve
                and its parabolic approximation, increasing the vertex shift.
            </p>

            <p>
                Because the IsoFLOP parabola is fit in \(\log N\) space (as described in the
                Approach 2 procedure), the vertex shift directly biases \(N^*\). Since
                \(C = 6ND\), analyzing the bias in either \(N^*\) or \(D^*\) is
                sufficient; we focus on \(N^*\) here since that is where the parabolic
                fit typically operates.
            </p>

            <p>
                Since the vertex shift is constant across all compute budgets, it biases every
                inferred \(N^*\) by the same multiplicative factor. When fitting
                \(\log N^*\) vs \(\log C\) to extract scaling exponents:
            </p>

            <ul>
                <li>The <strong>slope (exponent)</strong> is unchanged: multiplying all \(N^*\) values
                    by a constant factor adds a constant to \(\log N^*\), which doesn't affect the slope</li>
                <li>The <strong>intercept</strong> absorbs the entire error, biased by exactly
                    that multiplicative factor</li>
            </ul>

            <p>
                <strong>Exact derivation:</strong> The intercept error can be derived analytically
                in closed form. The parabola vertex shifts by \(\delta w\) (in log-space), giving
                an intercept error of:
            </p>

            <div class="math-block">
                \[
                \text{Intercept error} = 10^{\delta w} - 1
                \]
            </div>

            <p>
                where \(\delta w = f(\alpha, \beta, W, n)\) depends only on the surface exponents
                and the sampling grid (width \(W\) in log-space, number of points \(n\) per IsoFLOP curve), not on \(C\), \(E\),
                \(A\), or \(B\). Here \(W\) spans \(10^{-W/2}\) to \(10^{W/2}\) times the optimal \(N^*\),
                so \(W = 2.41\) (the XL grid) means sampling from \(\frac{1}{16}\times\) to \(16\times\) the optimum.
                And \(n = 15\) means 15 model sizes per compute budget. Key properties:
            </p>

            <ul>
                <li>\(\delta w = 0\) when \(\alpha = \beta\) (symmetric surfaces have no error)</li>
                <li>\(\delta w\) grows with \(|\alpha - \beta|\) (more asymmetry ‚Üí more error)</li>
                <li>\(\delta w\) grows with \(W\) (wider sampling range ‚Üí more error)</li>
            </ul>

            <p>
                For example, with the Chinchilla parameters (\(\alpha = 0.34\), \(\beta = 0.28\)):
                the XS grid (\(W = 0.60\)) yields 0.3% intercept error, while the XL grid
                (\(W = 2.41\)) yields 4.1% error.
            </p>

            <p>
                The <a href="https://github.com/Open-Athena/scaling-law-analysis/blob/main/results/article/static/scaling_parameter_errors.pdf">full derivation</a> provides the
                closed-form expression for vertex shift \(\delta w\) as a function of \(\alpha\),
                \(\beta\), \(W\), and \(n\). It also shows how this shift translates directly
                into intercept error, independent of compute budget.
            </p>

            <p>
                <strong>Intuition via Taylor expansion:</strong> A parabola is a 2nd-order polynomial,
                which is equivalent to a 2nd-order Taylor expansion around the optimum. The approximation
                \(L(w) \approx L(0) + \frac{1}{2}L''(0)w^2\) is only valid when higher-order terms are
                negligible, i.e., when samples are close to the true minimum. As sampling range increases,
                3rd and 4th order terms grow. For symmetric surfaces (\(\alpha = \beta\)), odd-order
                terms cancel by symmetry, preserving the vertex location. For asymmetric surfaces,
                they don't cancel, shifting the fitted vertex away from the true optimum.
            </p>

            <h3 id="why-it-matters">Why It Matters</h3>

            <p>
                Extrapolation to higher compute budgets requires both exponents and intercepts to be correct.
                The <a href="#why-it-happens">previous section</a> established that asymmetric loss surfaces produce provably biased
                intercepts even under ideal experimental conditions. Here we quantify what those errors
                mean in practical terms by examining compute-optimal token prediction: given a compute
                budget, how many tokens does the inferred scaling law predict?
            </p>

            <p>
                Up to this point, all analysis has assumed a single fixed sampling grid width.
                We now examine how token prediction error varies with both compute budget and
                sampling grid width. For surfaces with asymmetric exponents, wider sampling grids
                amplify the parabola-fitting mismatch, increasing the constant vertex shift and
                thus the intercept bias. To make this comparison concrete, we first define what
                "wider" and "narrower" mean in quantitative terms.
            </p>

            <p>
                A sampling grid of "¬±kx" means the sampled values (whether
                model sizes or token counts) range from <sup>1</sup>&frasl;<sub>k</sub> to k times the true optimum at
                each compute budget. The total range covered is k¬≤ (the ratio of largest to
                smallest), and the log‚ÇÅ‚ÇÄ of that ratio tells you how many factors of
                10, or "decades," the grid spans end-to-end (e.g. a value of 1.81 means
                the largest sample is 10<sup>1.81</sup> ‚âà 64x the smallest). The table
                below shows the four grid widths used in this analysis:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Grid Name</th>
                        <th>¬±kx</th>
                        <th>Sampling Range</th>
                        <th>Total Ratio</th>
                        <th>Decade Span (factors of 10)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Extra Small (XS)</td>
                        <td>¬±2x</td>
                        <td>1/2x to 2x</td>
                        <td>4x</td>
                        <td>0.60</td>
                    </tr>
                    <tr>
                        <td>Small (S)</td>
                        <td>¬±4x</td>
                        <td>1/4x to 4x</td>
                        <td>16x</td>
                        <td>1.20</td>
                    </tr>
                    <tr>
                        <td>Large (L)</td>
                        <td>¬±8x</td>
                        <td>1/8x to 8x</td>
                        <td>64x</td>
                        <td>1.81</td>
                    </tr>
                    <tr>
                        <td>Extra Large (XL)</td>
                        <td>¬±16x</td>
                        <td>1/16x to 16x</td>
                        <td>256x</td>
                        <td>2.41</td>
                    </tr>
                </tbody>
            </table>

            <p>
                In practice, scaling law experiments typically sample across 1 to 2 decades in
                token count, placing the Small and Large grids squarely within the realistic range.
                The Extra Small and Extra Large grids bracket this range on either side, illustrating
                how the biases shrink or grow as the sampling window narrows or widens. The Extra
                Large grid (¬±16x, ~2.4 decades) is the default used in all single-grid
                analyses in the preceding sections.
            </p>

            <figure>
                <img src="figures/extrapolation_error/extrapolation_error.png" alt="Bar chart showing token prediction error by surface and grid width">
                <figcaption>
                    <strong>Figure 3:</strong> Relative error in compute-optimal token prediction when
                    extrapolating from the training range (10¬π‚Å∑‚Äì10¬≤¬π FLOPs) to 10¬≤‚Å¥ FLOPs. Negative values
                    indicate underestimation: the inferred scaling law predicts fewer tokens than optimal.
                    Bars are grouped by sampling grid width. Annotations for the Chinchilla surface
                    show \(D^*\) (true compute-optimal token count) versus \(\hat{D}^*\) (the
                    Approach 2 estimate); the Small and Large grid annotations are emphasized (thicker borders) as
                    they fall within the realistic 1‚Äì2 decade range typical of scaling law experiments, while
                    Extra Small and Extra Large bracket either side as more extreme configurations.
                </figcaption>
            </figure>

            <details class="data-table-container">
                <summary>üìä View raw data</summary>
                <div class="data-table-wrapper">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Surface</th>
                                <th>Œ±</th>
                                <th>Œ≤</th>
                                <th>Grid</th>
                                <th>True D*</th>
                                <th>Inferred D*</th>
                                <th>Abs Error</th>
                                <th>Rel Error</th>
                            </tr>
                        </thead>
                        <tbody>
                            <!-- Symmetric Surface -->
                            <tr class="surface-header">
                                <td colspan="8">Symmetric Surface (Œ± = Œ≤)</td>
                            </tr>
                            <tr>
                                <td>Symmetric</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>XS (¬±2√ó)</td>
                                <td title="4.082482904638630e+11">408.2B</td>
                                <td title="4.082482904640633e+11">408.2B</td>
                                <td title="2.002563476562500e-01">‚âà0</td>
                                <td title="0.000000000049053%" class="error-zero">‚âà0%</td>
                            </tr>
                            <tr>
                                <td>Symmetric</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>S (¬±4√ó)</td>
                                <td title="4.082482904638630e+11">408.2B</td>
                                <td title="4.082482904638362e+11">408.2B</td>
                                <td title="-2.685546875000000e-02">‚âà0</td>
                                <td title="-0.000000000006578%" class="error-zero">‚âà0%</td>
                            </tr>
                            <tr>
                                <td>Symmetric</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>L (¬±8√ó)</td>
                                <td title="4.082482904638630e+11">408.2B</td>
                                <td title="4.082482904638746e+11">408.2B</td>
                                <td title="1.153564453125000e-02">‚âà0</td>
                                <td title="0.000000000002826%" class="error-zero">‚âà0%</td>
                            </tr>
                            <tr>
                                <td>Symmetric</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>XL (¬±16√ó)</td>
                                <td title="4.082482904638630e+11">408.2B</td>
                                <td title="4.082482904640399e+11">408.2B</td>
                                <td title="1.768798828125000e-01">‚âà0</td>
                                <td title="0.000000000043327%" class="error-zero">‚âà0%</td>
                            </tr>
                            <!-- Chinchilla Surface -->
                            <tr class="surface-header">
                                <td colspan="8">Chinchilla Surface (Œ± ‚â† Œ≤)</td>
                            </tr>
                            <tr>
                                <td>Chinchilla</td>
                                <td>0.34</td>
                                <td>0.28</td>
                                <td>XS (¬±2√ó)</td>
                                <td title="4.035834749563681e+12">4.04T</td>
                                <td title="4.022640080071175e+12">4.02T</td>
                                <td title="-1.319466949250537e+10">‚àí13.2B</td>
                                <td title="-0.326937803732719%" class="error-low">‚àí0.33%</td>
                            </tr>
                            <tr>
                                <td>Chinchilla</td>
                                <td>0.34</td>
                                <td>0.28</td>
                                <td>S (¬±4√ó)</td>
                                <td title="4.035834749563681e+12">4.04T</td>
                                <td title="3.983323492914587e+12">3.98T</td>
                                <td title="-5.251125664909375e+10">‚àí52.5B</td>
                                <td title="-1.301125043704300%" class="error-low">‚àí1.30%</td>
                            </tr>
                            <tr>
                                <td>Chinchilla</td>
                                <td>0.34</td>
                                <td>0.28</td>
                                <td>L (¬±8√ó)</td>
                                <td title="4.035834749563681e+12">4.04T</td>
                                <td title="3.918678311395280e+12">3.92T</td>
                                <td title="-1.171564381684004e+11">‚àí117.2B</td>
                                <td title="-2.902904738135434%" class="error-medium">‚àí2.90%</td>
                            </tr>
                            <tr>
                                <td>Chinchilla</td>
                                <td>0.34</td>
                                <td>0.28</td>
                                <td>XL (¬±16√ó)</td>
                                <td title="4.035834749563681e+12">4.04T</td>
                                <td title="3.829997307632958e+12">3.83T</td>
                                <td title="-2.058374419307222e+11">‚àí205.8B</td>
                                <td title="-5.100244551712022%" class="error-medium">‚àí5.10%</td>
                            </tr>
                            <!-- Asymmetric Surface -->
                            <tr class="surface-header">
                                <td colspan="8">Asymmetric Surface (Œ±/Œ≤ = 3)</td>
                            </tr>
                            <tr>
                                <td>Asymmetric</td>
                                <td>0.465</td>
                                <td>0.155</td>
                                <td>XS (¬±2√ó)</td>
                                <td title="4.510334355952566e+16">45.1Q</td>
                                <td title="4.434797634319416e+16">44.3Q</td>
                                <td title="-7.553672163314960e+14">‚àí755.4T</td>
                                <td title="-1.674747716507073%" class="error-low">‚àí1.67%</td>
                            </tr>
                            <tr>
                                <td>Asymmetric</td>
                                <td>0.465</td>
                                <td>0.155</td>
                                <td>S (¬±4√ó)</td>
                                <td title="4.510334355952566e+16">45.1Q</td>
                                <td title="4.217262808974590e+16">42.2Q</td>
                                <td title="-2.930715469779760e+15">‚àí2.9Q</td>
                                <td title="-6.497778742083532%" class="error-medium">‚àí6.50%</td>
                            </tr>
                            <tr>
                                <td>Asymmetric</td>
                                <td>0.465</td>
                                <td>0.155</td>
                                <td>L (¬±8√ó)</td>
                                <td title="4.510334355952566e+16">45.1Q</td>
                                <td title="3.882927967488314e+16">38.8Q</td>
                                <td title="-6.274063884642520e+15">‚àí6.3Q</td>
                                <td title="-13.910418584294648%" class="error-high">‚àí13.91%</td>
                            </tr>
                            <tr>
                                <td>Asymmetric</td>
                                <td>0.465</td>
                                <td>0.155</td>
                                <td>XL (¬±16√ó)</td>
                                <td title="4.510334355952566e+16">45.1Q</td>
                                <td title="3.467711572211202e+16">34.7Q</td>
                                <td title="-1.042622783741363e+16">‚àí10.4Q</td>
                                <td title="-23.116308048545221%" class="error-high">‚àí23.12%</td>
                            </tr>
                        </tbody>
                    </table>
                    <p style="margin-top: 0.75rem; font-size: 0.8rem; color: var(--text-muted);">
                        B = billion, T = trillion, Q = quadrillion. Hover over cells for full-precision values.
                        Training range: 10¬π‚Å∑‚Äì10¬≤¬π FLOPs. Evaluation budget: 10¬≤‚Å¥ FLOPs.
                    </p>
                </div>
            </details>

            <p>
                The key observations from this figure are:
            </p>

            <ul>
                <li><strong>Symmetric surfaces are unaffected:</strong> When \(\alpha = \beta\), all grid
                    widths produce zero error</li>
                <li><strong>Asymmetric surfaces underestimate:</strong> Negative errors mean the inferred
                    \(D^*\) is smaller than the true \(D^*\). Following these predictions would undertrain
                    the model</li>
                <li><strong>Wider grids amplify error:</strong> Moving from XS (¬±2x) to XL (¬±16x)
                    grids increases error from 0.3% to 5.1% on Chinchilla, and from 1.7% to 23% on the Asymmetric surface</li>
                <li><strong>Asymmetry magnifies everything:</strong> The Asymmetric surface
                    (\(\alpha/\beta = 3\)) shows roughly 4‚Äì5x larger errors than Chinchilla at each grid width</li>
            </ul>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    Consider the Chinchilla surface with the Large grid (¬±8x), a practical sampling
                    range for real experiments. When extrapolating to 10¬≤‚Å¥ FLOPs, the true optimal
                    token count is 4.04 trillion, but Approach 2 predicts only 3.92 trillion: a 2.9%
                    underestimate, or roughly 117 billion fewer tokens than optimal. While 2.9% may
                    seem modest, recall that this simulation uses unrealistically ideal conditions:
                    perfectly centered sampling grids at every compute budget and zero measurement
                    noise. Real experiments, where the true optimum is unknown, data is noisy,
                    and the scaling exponent imbalance may be larger than Chinchilla's modest
                    \(\alpha/\beta \approx 1.2\), can only do worse.
                </p>
            </div>

        </section>

        <!-- Section 4: Off-Center Sampling -->
        <section id="off-center">
            <h2>Off-Center Sampling: Exponent and Extrapolation Errors</h2>

            <p>
                The previous sections assumed perfectly centered sampling. At every compute budget,
                the IsoFLOP grid was placed exactly at the true optimum. In practice, you don't know
                \(N^*\) before running the experiment. Sampling centers are guesses, informed by prior
                estimates or heuristics, and they will likely be wrong by some amount.
            </p>

            <p>
                This is a distinct source of error from the asymmetry bias examined earlier. Asymmetry
                errors arise from the shape of the loss surface (\(\alpha \neq \beta\)); off-center
                errors arise from where you place the sampling grid. To isolate this new effect, we
                return to the symmetric surface (\(\alpha = \beta = 0.31\)) where asymmetry bias is
                zero by construction.
            </p>

            <h3 id="constant-bias">Constant Multiplicative Bias</h3>

            <p>
                The simplest form of off-center sampling is a constant multiplicative offset: every
                compute budget's sampling center is shifted by the same factor from the true optimum.
                A "3√ó offset" means each IsoFLOP grid is centered at \(3 \times D^*\) instead of
                \(D^*\), so the grid midpoint consistently sits at three times the true optimal token count.
            </p>

            <p>
                Because this offset is the same at every compute budget, it has a familiar geometric
                effect where each parabola vertex shifts by a constant amount in log-space. This is the
                same mechanism as asymmetry bias. The slope of \(\log D^*\) vs \(\log C\) is
                unaffected (a constant additive shift in log-space doesn't change the slope), so the
                scaling exponent is preserved perfectly. The intercept, however, absorbs the entire
                error.
            </p>

            <figure>
                <img src="figures/off_center_constant_bias/off_center_constant_bias.png"
                    alt="Off-center sampling with constant multiplicative bias showing zero exponent error but systematic intercept error">
                <figcaption>
                    <strong>Figure 4:</strong> Effect of a constant 3√ó offset in sampling centers on
                    the symmetric surface. Top left: IsoFLOP curves at the Large grid (¬±8√ó), with black
                    diamonds marking the (off-center) sampling center, red √ó the true \(D^*\), and
                    blue + the inferred \(D^*\). Top right: extrapolation error in compute-optimal
                    token prediction at 10¬≤‚Å¥ FLOPs for each grid width, using the same XS through XL
                    grids defined earlier. Bottom row: exponent and intercept errors across grid widths
                    from XS (¬±2√ó) to XL (¬±16√ó), plotted on the same y-axis scale. The exponent is
                    recovered perfectly (flat at zero) while the intercept shows systematic bias that
                    varies with grid width.
                </figcaption>
            </figure>

            <p>
                The extrapolation bar chart (top right) shows what this means for token prediction.
                All four grid widths overestimate \(D^*\), with the narrowest grid (XS) producing the
                largest error. This is the reverse of the asymmetry bias pattern, where wider grids
                amplified error. Here, narrower grids are more sensitive to off-center placement because
                fewer samples lie near the true optimum.
            </p>

            <p>
                The intercept error panel (bottom right) confirms the pattern across the full
                continuum of grid widths. The error is always positive (the inferred \(D^*\)
                overshoots) and decreases monotonically as the grid widens, reflecting how a wider
                sampling range brings more of the true loss curve's shape into the fit, partially
                compensating for the misplaced center.
            </p>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    Consider the symmetric surface with the Large grid (¬±8√ó) and a 3√ó offset,
                    where every IsoFLOP grid is centered at three times the true optimal token count.
                    When extrapolating to 10¬≤‚Å¥ FLOPs, the true optimal token count is 408.2 billion,
                    but Approach 2 predicts 419.0 billion: a 2.6% overestimate, roughly 10.8 billion
                    more tokens than optimal. Compare this with the Chinchilla asymmetry result at the
                    same grid width: a 2.9% underestimate. The magnitudes are comparable,
                    but the sources are entirely different. Asymmetry bias comes from the
                    shape of the loss surface; off-center bias comes from where you place the grid.
                    In a real experiment, both act simultaneously.
                </p>
            </div>

            <h3 id="drifting-bias">Drifting Bias</h3>

            <p>
                When the offset varies with compute budget, a qualitatively different failure
                mode emerges. To illustrate this, we apply a linear drift. The sampling center
                starts at the true optimum for the lowest budget and drifts to 3√ó the true
                optimum at the highest budget, interpolating linearly in log-compute space.
            </p>

            <p>
                Because the offset now differs across compute budgets, it no longer cancels
                in the slope of \(\log D^*\) vs \(\log C\). Both the exponent and the
                intercept are affected.
            </p>

            <figure>
                <img src="figures/off_center_drifting_bias/off_center_drifting_bias.png"
                    alt="Off-center sampling with drifting bias showing both exponent and intercept errors">
                <figcaption>
                    <strong>Figure 5:</strong> Effect of a linear drift in sampling centers
                    (centered at true optimum for lowest budget, drifting to 3√ó at highest budget)
                    on the symmetric surface. Unlike the constant bias case, the exponent error
                    (bottom left) is now non-zero: the slope of \(\log D^*\) vs \(\log C\) is
                    distorted because the offset varies across compute budgets.
                </figcaption>
            </figure>

            <p>
                Compare the bottom-left panels of Figures 4 and 5: constant bias produces a
                flat line at zero (exponent preserved), while drifting bias produces
                a non-zero exponent error that varies with grid width.
            </p>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Message</div>
                <p>
                    Constant bias preserves exponents; any compute-dependent bias pattern distorts
                    them. The distinction matters because exponent errors compound during extrapolation,
                    while intercept errors remain fixed.
                </p>
            </div>

        </section>

        <!-- Section 5: IsoFLOP Curves in the Wild -->
        <section id="in-the-wild">
            <h2>IsoFLOP Curves in the Wild: Evidence from Published Studies</h2>

            <p>
                The previous sections used synthetic, noise-free simulations to isolate Approach 2's
                biases under controlled conditions. A natural question is whether the conditions that
                trigger these biases, asymmetric loss surfaces and imperfectly centered sampling,
                actually arise in practice. To get a sense of this, we can look at IsoFLOP curves
                published in three of the most prominent scaling law
                studies<sup><a href="#ref-chinchilla">[1]</a>,<a href="#ref-llama3">[2]</a>,<a href="#ref-deepseek">[3]</a></sup>.
            </p>

            <figure>
                <img src="static/isoflop_curve_examples.png"
                    alt="IsoFLOP curves from Chinchilla, Llama 3, and DeepSeek scaling law papers">
                <figcaption>
                    <strong>Figure 6:</strong> IsoFLOP curves from three published scaling law studies.
                    Left: Chinchilla (training loss vs parameters). Center: Llama 3 (validation loss vs
                    training tokens). Right: DeepSeek (bits-per-byte vs FLOPs/token). Each panel shows
                    curves at multiple compute budgets, fit using Approach 2.
                </figcaption>
            </figure>

            <p>
                Published scaling law studies frequently report asymmetric loss
                surfaces, as shown in (TODO: compile reported scaling exponents from
                studies spanning text, images, proteins, and DNA). Some degree of
                asymmetry is nearly universal across modalities, and several reported
                exponent estimates reach the same levels of imbalance as the most
                extreme configuration in our simulations. The related biases documented
                in this article therefore apply broadly.
            </p>

            <p>
                Beyond surface asymmetry, the IsoFLOP curves in Figure 6 also show visible
                signs of off-center sampling and drift:
            </p>

            <ul>
                <li><strong>Off-center sampling:</strong> At some compute budgets, the sampling grid
                    does not appear centered at the curve minimum, placing more points on one side
                    of the optimum than the other.</li>
                <li><strong>Drifting centers:</strong> The degree of off-centering appears to vary
                    across compute budgets rather than remaining constant, which is the
                    drifting-bias pattern that distorts both exponents and intercepts.</li>
            </ul>

            <p>
                To be clear, this is not a criticism of these studies. These are among the most careful
                and influential scaling law analyses published. The point is a more general one: the
                conditions under which Approach 2's biases activate, asymmetric surfaces and imperfect
                sampling centers, appear to be the norm rather than the exception. The idealized
                conditions of the <a href="#happy-path">Happy Path</a> (symmetric surface, perfectly centered grids) are the
                special case.
            </p>

            <h3 id="putting-it-together">Compounding Errors</h3>

            <p>
                Given evidence that both surface asymmetry and off-center sampling are present
                in real studies, we can simulate what happens when these biases act
                simultaneously. Using the same three loss surfaces from earlier sections, we
                combine them with the 3√ó drift and 3√ó constant offset from the off-center
                analysis. We fit Approach 2 on compute budgets from 10<sup>17</sup> to
                10<sup>21</sup> FLOPs and extrapolate \(D^*\) predictions to 10<sup>24</sup>
                FLOPs across all four grid widths.
            </p>

            <figure>
                <img src="figures/compounding_errors/compounding_errors.png"
                    alt="Compounding errors: D* prediction error under drift and constant offset sampling biases across three surfaces and four grid widths">
                <figcaption>
                    <strong>Figure 7:</strong> Relative error in \(D^*\) at 10<sup>24</sup> FLOPs
                    with off-center sampling on all three loss surfaces. Left: constant 3√ó
                    center offset at every budget. Right: linear drift to 3√ó at the highest
                    compute budget. Bars are grouped by sampling grid width (XS through XL).
                    Negative values indicate underestimation; positive values indicate
                    overestimation. On the symmetric surface, the constant offset results
                    correspond to Figure 4 and the drift results correspond to Figure 5;
                    the asymmetric surfaces reveal how these sampling biases interact with
                    the inherent asymmetry bias.
                </figcaption>
            </figure>

            <details class="data-table-container">
                <summary>üìä View raw data</summary>
                <div class="data-table-wrapper">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Config</th>
                                <th>Surface</th>
                                <th>Grid</th>
                                <th>True D*</th>
                                <th>Inferred D*</th>
                                <th>Rel Error</th>
                            </tr>
                        </thead>
                        <tbody>
                            <!-- Offset 3√ó -->
                            <tr class="surface-header">
                                <td colspan="6">Offset 3√ó (sampling center at 3√ó true optimum at every budget)</td>
                            </tr>
                            <tr><td>Offset 3√ó</td><td>Symmetric</td><td>XS (¬±2√ó)</td><td title="4.082482904638630e+11">408.2B</td><td title="4.244730718437291e+11">424.5B</td><td title="+3.974243556888148%" class="error-low">+3.97%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Symmetric</td><td>S (¬±4√ó)</td><td title="4.082482904638630e+11">408.2B</td><td title="4.224219276128955e+11">422.4B</td><td title="+3.471817881448564%" class="error-low">+3.47%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Symmetric</td><td>L (¬±8√ó)</td><td title="4.082482904638630e+11">408.2B</td><td title="4.190471331132362e+11">419.0B</td><td title="+2.645165430356910%" class="error-medium">+2.65%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Symmetric</td><td>XL (¬±16√ó)</td><td title="4.082482904638630e+11">408.2B</td><td title="4.144119915542275e+11">414.4B</td><td title="+1.509792259842919%" class="error-low">+1.51%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Chinchilla</td><td>XS (¬±2√ó)</td><td title="4.035834749563681e+12">4.04T</td><td title="4.322787400631939e+12">4.32T</td><td title="+7.110118943776913%" class="error-medium">+7.11%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Chinchilla</td><td>S (¬±4√ó)</td><td title="4.035834749563681e+12">4.04T</td><td title="4.265511774986103e+12">4.27T</td><td title="+5.690942262867751%" class="error-medium">+5.69%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Chinchilla</td><td>L (¬±8√ó)</td><td title="4.035834749563681e+12">4.04T</td><td title="4.172088880897259e+12">4.17T</td><td title="+3.376107789059233%" class="error-low">+3.38%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Chinchilla</td><td>XL (¬±16√ó)</td><td title="4.035834749563681e+12">4.04T</td><td title="4.045435227510737e+12">4.05T</td><td title="+0.237880848518242%" class="error-low">+0.24%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Asymmetric</td><td>XS (¬±2√ó)</td><td title="4.510334355952566e+16">45.1Q</td><td title="5.377273818771581e+16">53.8Q</td><td title="+19.221179504682659%" class="error-high">+19.22%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Asymmetric</td><td>S (¬±4√ó)</td><td title="4.510334355952566e+16">45.1Q</td><td title="5.160438683341485e+16">51.6Q</td><td title="+14.413661517819328%" class="error-high">+14.41%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Asymmetric</td><td>L (¬±8√ó)</td><td title="4.510334355952566e+16">45.1Q</td><td title="4.824225048547561e+16">48.2Q</td><td title="+6.959366375593294%" class="error-medium">+6.96%</td></tr>
                            <tr><td>Offset 3√ó</td><td>Asymmetric</td><td>XL (¬±16√ó)</td><td title="4.510334355952566e+16">45.1Q</td><td title="4.400977998376982e+16">44.0Q</td><td title="-2.424573190039884%" class="error-medium">‚àí2.42%</td></tr>
                            <!-- Drift to 3√ó -->
                            <tr class="surface-header">
                                <td colspan="6">Drift to 3√ó (sampling center drifts from true optimum to 3√ó at highest budget)</td>
                            </tr>
                            <tr><td>Drift to 3√ó</td><td>Symmetric</td><td>XS (¬±2√ó)</td><td title="4.082482904638630e+11">408.2B</td><td title="4.330329296629141e+11">433.0B</td><td title="+6.070971949665736%" class="error-medium">+6.07%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Symmetric</td><td>S (¬±4√ó)</td><td title="4.082482904638630e+11">408.2B</td><td title="4.293594960061901e+11">429.4B</td><td title="+5.171168143371758%" class="error-medium">+5.17%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Symmetric</td><td>L (¬±8√ó)</td><td title="4.082482904638630e+11">408.2B</td><td title="4.233449170507438e+11">423.3B</td><td title="+3.697903197519233%" class="error-low">+3.70%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Symmetric</td><td>XL (¬±16√ó)</td><td title="4.082482904638630e+11">408.2B</td><td title="4.151438757202593e+11">415.1B</td><td title="+1.689066535602950%" class="error-low">+1.69%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Chinchilla</td><td>XS (¬±2√ó)</td><td title="4.035834749563681e+12">4.04T</td><td title="4.504566829000340e+12">4.50T</td><td title="+11.614253519357661%" class="error-high">+11.61%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Chinchilla</td><td>S (¬±4√ó)</td><td title="4.035834749563681e+12">4.04T</td><td title="4.432623377869933e+12">4.43T</td><td title="+9.831637143942757%" class="error-high">+9.83%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Chinchilla</td><td>L (¬±8√ó)</td><td title="4.035834749563681e+12">4.04T</td><td title="4.315883700538125e+12">4.32T</td><td title="+6.939058914756627%" class="error-medium">+6.94%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Chinchilla</td><td>XL (¬±16√ó)</td><td title="4.035834749563681e+12">4.04T</td><td title="4.158830754490464e+12">4.16T</td><td title="+3.047597648542992%" class="error-medium">+3.05%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Asymmetric</td><td>XS (¬±2√ó)</td><td title="4.510334355952566e+16">45.1Q</td><td title="6.069765266782697e+16">60.7Q</td><td title="+34.574618814502173%" class="error-high">+34.57%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Asymmetric</td><td>S (¬±4√ó)</td><td title="4.510334355952566e+16">45.1Q</td><td title="5.865168157587590e+16">58.7Q</td><td title="+30.038433843534605%" class="error-high">+30.04%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Asymmetric</td><td>L (¬±8√ó)</td><td title="4.510334355952566e+16">45.1Q</td><td title="5.546299036327943e+16">55.5Q</td><td title="+22.968689206115091%" class="error-high">+22.97%</td></tr>
                            <tr><td>Drift to 3√ó</td><td>Asymmetric</td><td>XL (¬±16√ó)</td><td title="4.510334355952566e+16">45.1Q</td><td title="5.141644796779622e+16">51.4Q</td><td title="+13.996976521128124%" class="error-high">+14.00%</td></tr>
                        </tbody>
                    </table>
                    <p style="margin-top: 0.75rem; font-size: 0.8rem; color: var(--text-muted);">
                        B = billion, T = trillion, Q = quadrillion. Hover over cells for full-precision values.
                        Training range: 10¬π‚Å∑‚Äì10¬≤¬π FLOPs. Evaluation budget: 10¬≤‚Å¥ FLOPs.
                    </p>
                </div>
            </details>

            <p>
                Comparing with the baseline in Figure 3, where asymmetry bias alone produces
                errors up to ‚àí5% on Chinchilla and ‚àí23% on the Asymmetric surface, the two
                bias sources interact in opposite directions. Off-center sampling pushes
                errors positive (overestimating \(D^*\)), while asymmetry bias pushes errors
                negative (underestimating). The net error depends on which source dominates.
                With narrow grids, asymmetry bias is negligible and the sampling bias determines
                the error: drift to 3√ó produces +6% on the symmetric surface and +12% on
                Chinchilla. With wider grids, asymmetry bias grows and begins to offset the
                sampling bias. On Chinchilla with a constant 3√ó offset, this cancellation
                is nearly perfect with the XL grid (+0.24%), but this is only coincidental.
            </p>

            <p>
                On the Asymmetric surface, the drift configuration produces the largest errors
                in the figure: +35% with the XS grid and still +14% with XL. Even the
                constant offset configuration reaches +19% with XS before the asymmetry
                bias partially offsets it with wider grids.
            </p>

            <p>
                These 3√ó perturbations are representative of realistic conditions. The IsoFLOP curves they
                produce on the symmetric surface (top-left panels of Figures 4 and 5) show
                sampling centers that are visibly displaced from the curve minima, with the
                displacement either uniform across budgets (constant offset) or growing toward
                higher budgets (drift). Both patterns are qualitatively similar to what is
                observed in the published studies shown in Figure 6, where sampling grids
                are not perfectly centered and the degree of off-centering varies across
                compute budgets. A 3√ó factor means the sampling center sits at three times
                the true optimal token count, which is likely within the range of uncertainty
                practitioners face when choosing sampling centers before the optimum is
                known.
            </p>

            <p>
                <a href="#appendix-combined-extrapolation">Figure A2</a> provides a more
                detailed view: it shows how \(D^*\) extrapolation errors evolve across
                compute budgets from 10<sup>22</sup> to 10<sup>25</sup> FLOPs, revealing
                which bias sources produce errors that grow with extrapolation distance
                (drift) versus those that remain roughly constant (surface asymmetry and
                constant offsets), and how these patterns vary across multiple drift rates
                and center offset magnitudes.
            </p>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    Multiple bias sources act simultaneously in any real experiment. Surface
                    asymmetry and off-center sampling each produce meaningful errors on their
                    own. When they happen to act in the same direction, the combined error
                    exceeds either one alone: on the Asymmetric surface with drift to 3√ó,
                    errors reach 35% even when using the narrowest grid, where the parabolic
                    approximation is most accurate. When they oppose, partial cancellation
                    can occur, but this depends on the specific combination of surface
                    geometry, offset magnitude, and grid width, making it unreliable in
                    practice.
                </p>
            </div>

        </section>

        <!-- Section 6: Robust Fits -->
        <section id="robust-fits">
            <h2>Robust Fits: Unbiased Estimation with Linear Separation</h2>

            <p>
                The previous sections showed that Approach 2's parabolic approximation introduces
                systematic biases in intercepts (from asymmetry) and potentially exponents (from
                off-center sampling), and that the conditions driving these biases are visible
                in published scaling law studies. The natural alternative is Approach 3, which fits all five
                surface parameters \((E, A, B, \alpha, \beta)\) simultaneously via nonlinear
                least squares. This avoids the parabolic approximation entirely but brings its
                own set of problems.
            </p>

            <h3 id="approach-3-problems">Problems with Direct Surface Fitting</h3>

            <p>
                A recent survey of over 50 scaling law
                papers<sup><a href="#ref-misfitting">[13]</a></sup> documents the landscape of
                fitting practices and their failure modes. The problems described below apply to
                scaling law fitting in general, not just Chinchilla forms, but they are directly
                relevant because Approach 3 involves the same kind of nonlinear optimization.
                Over half of the papers surveyed do not fully specify their fitting procedure
                (optimizer, loss function, or initialization), which compounds reproducibility
                challenges.
            </p>

            <p>
                The most common optimizers for scaling law fits are BFGS and L-BFGS. Some
                studies use SGD-family optimizers like Adam and Adagrad, though these are noted
                as sometimes poorly suited for curve fitting due to limited data efficiency. At
                least one study<sup><a href="#ref-data_filtering_scaling">[14]</a></sup>
                forgoes optimization entirely in favor of pure grid search
                because fitted solutions are too unstable.
            </p>

            <p>
                In practice, this instability takes several forms. Results are sensitive to
                initialization: different starting points for the optimizer can lead to
                substantially different fitted parameters. Results are also sensitive to
                optimizer hyperparameters such as convergence tolerance and gradient estimation
                method. And the optimizer frequently converges to local minima rather than the
                global optimum.
            </p>

            <p>
                Initialization is the most studied source of variability. Common mitigations
                include grid search over thousands of starting points (running the optimizer
                from each and keeping the best fit), random sampling of starting points,
                evaluating a coarse grid without optimization and seeding the optimizer from
                the single best candidate, or initializing from previously published parameter
                values. Yet the survey's own experiments
                show that full-grid optimization over 4500 starting points can yield results that
                diverge significantly from reported figures, evidence of "the difficulty of
                optimizing over this space, and the presence of many local minima."
            </p>

            <p>
                A simpler alternative is to log-linearize the power law and fit with linear
                regression. However, the log transformation changes the error distribution and
                exaggerates errors at small loss values, biasing parameter estimates. This bias
                is easily observed in simulations like ours. The survey also finds that the
                choice of loss function (whether Log-Huber, Huber, MSE, or MAE) affects fitted
                parameters unpredictably across datasets, and non-MSE objectives can introduce
                systematic bias in parameter estimates. Our goal is to identify a fitting method
                that is simple, stable, and efficient rather than to address outliers or other
                statistical concerns, so we use MSE for all fits in this article.
            </p>

            <p>
                The survey's experimental analysis varies optimizer, loss function, and
                initialization strategy across three datasets. The overarching finding is that
                none of these choices reliably eliminates instability, and results shift
                unpredictably between datasets. A key contributor is the high dimensionality of
                the joint five-parameter optimization, which creates a complex loss landscape
                with many local minima and interacting sensitivities. Reducing the dimensionality
                of the nonlinear search is one way to make the problem more tractable.
            </p>

            <p>
                As an example of what "complex loss landscape" means concretely, consider the
                Hessian of the residual sum of squares (RSS) objective for a five-parameter
                fit on noise-free data from the Asymmetric surface
                (\(\alpha = 0.465\), \(\beta = 0.155\)), using five IsoFLOP contours from
                \(10^{17}\) to \(10^{21}\) FLOPs with 15 points per curve.
                Its eigenvalues reveal how sensitive the objective is to perturbations along
                each parameter
                direction<sup><a href="#ref-hessian_optimization">[28]</a></sup>,
                and the condition number \(\kappa\) (the ratio of
                the largest to the smallest eigenvalue) measures how difficult the landscape
                is for gradient-based methods to navigate. For this surface, the five
                eigenvalues span from approximately \(8 \times 10^{-6}\) to
                \(3 \times 10^{6}\), giving \(\kappa \approx 3.5 \times 10^{11}\). The two
                flattest directions (smallest eigenvalues) point almost entirely along the
                linear coefficients \(A\) and \(B\). Near the optimum, perturbing either
                coefficient barely changes the RSS, making them effectively underdetermined
                by the data even when the data are perfect. The steepest directions are
                dominated by the scaling exponents \(\alpha\) and \(\beta\).
            </p>

            <p>
                Quasi-Newton methods like L-BFGS, which are among the most common optimizers
                for scaling law fits, build an approximate inverse Hessian to scale gradient
                steps across parameter directions. When eigenvalues span 12 orders of
                magnitude, the gradient signal along the flat \(A\)/\(B\) directions is
                negligible compared to the steep \(\alpha\)/\(\beta\) directions, and
                convergence criteria are often satisfied by progress in the steep directions
                before the flat directions are resolved. Separating the linear parameters
                from the nonlinear search eliminates the ill-conditioned directions entirely.
                The resulting two-dimensional landscape over \((\alpha, \beta)\) has a Hessian
                condition number of \(\kappa \approx 11\) in our example, a reduction by a factor of roughly
                \(3 \times 10^{10}\). This motivates an algorithm that exploits the partially
                linear structure of the Chinchilla loss surface to search only the
                well-conditioned two-dimensional subspace.
            </p>

            <h3 id="vpnls">Variable Projection (VPNLS)</h3>

            <p>
                The Chinchilla loss surface has a partially linear structure that can be
                exploited. For any fixed values of \(\alpha\) and \(\beta\), the remaining
                parameters \((E, A, B)\) enter the model linearly and can be solved exactly
                via least squares. This is the same computational shortcut that motivates
                Approach 2 (optimizing exponential terms separately from linear terms), but
                applied here without the parabolic approximation.
            </p>

            <p>
                The algorithm searches over \((\alpha, \beta)\) and, at each candidate pair,
                solves for \((E, A, B)\) via non-negative least squares (NNLS). A coarse
                32&times;32 grid search identifies a good starting region, and a Nelder-Mead
                simplex optimizer refines it. The linear separation is maintained throughout.
                The optimizer only ever navigates the two-dimensional \((\alpha, \beta)\)
                surface, never the full five-parameter space. We term this method Variable 
                Projection with Non-negative Least Squares (VPNLS).
            </p>

            <div class="math-block" style="font-family: 'JetBrains Mono', monospace; font-size: 0.88em; line-height: 1.8;">
<pre style="margin: 0; white-space: pre; overflow-x: auto;"><b>function</b> VPNLS(data):

    <b>function</b> objective(Œ±, Œ≤):
        X ‚Üê [1, N^(-Œ±), D^(-Œ≤)]         <span style="color: var(--text-muted);">// design matrix, one row per observation</span>
        (E, A, B) ‚Üê NNLS(X, L)          <span style="color: var(--text-muted);">// linear solve with E, A, B ‚â• 0</span>
        <b>return</b> ‚ÄñL ‚àí X¬∑[E, A, B]‚Äñ¬≤

    (Œ±‚ÇÄ, Œ≤‚ÇÄ) ‚Üê argmin objective(Œ±, Œ≤)  <span style="color: var(--text-muted);">// coarse 32√ó32 grid search</span>
    (Œ±*, Œ≤*) ‚Üê NelderMead(objective,    <span style="color: var(--text-muted);">// refine in 2D only</span>
                           start=(Œ±‚ÇÄ, Œ≤‚ÇÄ))
    (E*, A*, B*) ‚Üê NNLS(X(Œ±*, Œ≤*), L)  <span style="color: var(--text-muted);">// recover linear params at solution</span>

    <b>return</b> (E*, A*, B*, Œ±*, Œ≤*)</pre>
            </div>

            <p>
                The choice of Nelder-Mead over L-BFGS-B is deliberate. VPNLS uses NNLS for
                the inner solve to guarantee that \(E\), \(A\), and \(B\) remain non-negative,
                preventing physically meaningless fits. However, NNLS has no closed-form
                gradient with respect to the outer parameters \((\alpha, \beta)\). Switching to
                ordinary least squares would restore differentiability but cannot enforce
                non-negativity. With NNLS, L-BFGS-B must rely on finite-difference gradients,
                which creates a set of interacting tuning parameters
                (<code>eps</code>, <code>jac</code>, <code>ftol</code>, <code>gtol</code>,
                <code>maxcor</code>, <code>maxls</code>) where tight tolerances demand gradient
                accuracy that finite differences cannot reliably provide.
            </p>

            <p>
                Nelder-Mead avoids this entirely. Its few settings
                (<code>xatol</code>, <code>fatol</code>) are independent
                and work well out of the box. Nelder-Mead scales poorly to high dimensions,
                but variable projection reduces the search to just two dimensions, which is
                exactly the regime where simplex methods excel.
            </p>

            <h3 id="method-comparison-recovery">Method Comparison (Parameter Recovery)</h3>

            <p>
                To validate this choice, we compare nine method configurations on noise-free
                synthetic data across three loss surfaces (symmetric, Chinchilla, and high
                imbalance) and 20 sampling ranges. This is the best case for gradient-based
                methods since the data contains no noise that could obscure gradient information.
            </p>

            <p>
                The configurations fall into two groups. The first uses 5D direct optimization
                (Approach 3), fitting all five parameters jointly with L-BFGS-B using either
                analytical gradients, forward finite differences, or central finite differences.
                The second uses 2D variable projection over \((\alpha, \beta)\) only, comparing
                VPNLS (Nelder-Mead), L-BFGS-B with four finite-difference configurations
                (default \(\varepsilon\), central differences,
                \(\varepsilon = 10^{-6}\), and \(\varepsilon = 10^{-10}\)), and a fine
                256&sup2; grid search with no local refinement.
                Both groups use the same total initialization budget: the 5D methods search a
                4<sup>5</sup>&nbsp;=&nbsp;1,024-point grid over all five parameters, while the
                2D methods search a 32<sup>2</sup>&nbsp;=&nbsp;1,024-point grid over
                \((\alpha, \beta)\) only, so that accuracy differences are more likely to
                reflect the optimizer and loss-landscape geometry than an initialization advantage.
            </p>

            <figure>
                <img src="figures/parameter_recovery/parameter_recovery.png"
                    alt="Method comparison showing geometric mean error and max error across nine optimizer configurations">
                <figcaption>
                    <strong>Figure 8:</strong> Comparison of nine fitting methods on noise-free
                    synthetic data across three loss surfaces and 20 sampling ranges (60 fits
                    total per method). Left: geometric mean of |relative error| (%) pooled across
                    all surfaces, grid widths, and parameters, with horizontal bars spanning the
                    min-to-max range. Filled dots indicate convergence on all 60 fits; open dots
                    indicate at least one failure (count annotated). Right: maximum |relative
                    error| (%) per parameter over successful fits, on a log-scale colormap.
                    Methods are sorted by geometric mean error, with the worst at top.
                </figcaption>
            </figure>

            <details class="data-table-container">
                <summary>üìä View method comparison data</summary>
                <div class="data-table-wrapper">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Method</th>
                                <th>Failures</th>
                                <th>Max E err%</th>
                                <th>Max A err%</th>
                                <th>Max B err%</th>
                                <th>Max Œ± err%</th>
                                <th>Max Œ≤ err%</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">2D Nelder-Mead (VPNLS)</td>
                                <td class="error-zero">0/60</td>
                                <td>5.2√ó10‚Åª‚Å∏</td>
                                <td>6.3√ó10‚Åª‚Å∏</td>
                                <td>7.9√ó10‚Åª‚Å∏</td>
                                <td>1.2√ó10‚Åª‚Å∏</td>
                                <td>2.0√ó10‚Åª‚Å∏</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">2D L-BFGS-B (central diff)</td>
                                <td class="error-low">2/60</td>
                                <td>8.3√ó10‚Åª‚Å∏</td>
                                <td>5.3√ó10‚Åª‚Å∏</td>
                                <td>6.4√ó10‚Åª‚Å∏</td>
                                <td>1.2√ó10‚Åª‚Å∏</td>
                                <td>2.0√ó10‚Åª‚Å∏</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">2D L-BFGS-B (default Œµ)</td>
                                <td class="error-low">3/60</td>
                                <td>1.5√ó10‚Åª‚Åµ</td>
                                <td>1.2√ó10‚Åª‚Åµ</td>
                                <td>1.3√ó10‚Åª‚Åµ</td>
                                <td>2.7√ó10‚Åª‚Å∂</td>
                                <td>3.8√ó10‚Åª‚Å∂</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">2D L-BFGS-B (Œµ=10‚Åª¬π‚Å∞)</td>
                                <td class="error-zero">0/60</td>
                                <td>1.6√ó10‚Åª‚Å∑</td>
                                <td>8.9√ó10‚Åª‚Å∑</td>
                                <td>8.6√ó10‚Åª‚Å∑</td>
                                <td>1.8√ó10‚Åª‚Å∑</td>
                                <td>1.7√ó10‚Åª‚Å∑</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">2D L-BFGS-B (Œµ=10‚Åª‚Å∂)</td>
                                <td class="error-high">17/60</td>
                                <td>1.6√ó10‚Åª¬≥</td>
                                <td>1.1√ó10‚Åª¬≥</td>
                                <td>1.3√ó10‚Åª¬≥</td>
                                <td>2.2√ó10‚Åª‚Å¥</td>
                                <td>3.9√ó10‚Åª‚Å¥</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">2D Grid (256¬≤)</td>
                                <td class="error-zero">0/60</td>
                                <td>2.58</td>
                                <td>2.03</td>
                                <td>2.03</td>
                                <td>0.44</td>
                                <td>0.57</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">5D L-BFGS-B (analytical)</td>
                                <td class="error-zero">0/60</td>
                                <td>0.07</td>
                                <td>0.75</td>
                                <td>1.4</td>
                                <td>0.15</td>
                                <td>0.28</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">5D L-BFGS-B (central diff)</td>
                                <td class="error-low">2/60</td>
                                <td>39.0</td>
                                <td>2,361</td>
                                <td>57.9</td>
                                <td>69.8</td>
                                <td>9.6</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">5D L-BFGS-B (finite diff)</td>
                                <td class="error-low">4/60</td>
                                <td>132</td>
                                <td>2,361</td>
                                <td>2,335</td>
                                <td>71.4</td>
                                <td>87.6</td>
                            </tr>
                        </tbody>
                    </table>
                    <p style="margin-top: 0.75rem; font-size: 0.8rem; color: var(--text-muted);">
                        Maximum |relative error| (%) across 60 fits (3 surfaces √ó 20 sampling
                        ranges), computed over successful (converged) fits only. Failure counts
                        show convergence failures out of 60 total fits.
                    </p>
                </div>
            </details>

            <p>
                In the left panel, each dot shows the typical (geometric mean) parameter
                recovery error for one method, and the horizontal bar shows the range from best
                to worst case across 60 scenarios. The right panel breaks this down by parameter,
                showing the worst-case error for each.
            </p>

            <p>
                Consider the best Approach 3 configuration (5D L-BFGS-B with analytical
                gradients). Even with exact gradients on noise-free data, the worst-case errors
                reach about 1.4% for \(B\) and about 0.75% for \(A\), compared with
                VPNLS errors on the order of 10<sup>&minus;8</sup>% &mdash; a gap of
                roughly seven orders of magnitude.
                <a href="#appendix-method-comparison">Figure A1</a> breaks this down by
                surface and sampling range, also revealing that Approach 3's errors can vary
                systematically with sampling range on certain surfaces.
            </p>

            <p>
                Looking at the full set of methods, a clear hierarchy emerges. High-resolution
                grid search (256&sup2;) is stable across all conditions but provides the poorest
                overall precision among 2D methods, limited by grid resolution.
            </p>

            <p>
                5D direct optimization (Approach 3) is more accurate on average than grid search
                but highly variable across conditions. The 5D configurations that rely on
                finite-difference gradients rather than analytical gradients perform particularly
                poorly and serve as a useful negative control. They demonstrate what high
                variability and instability look like, and Approach 3 with analytical gradients
                exhibits a similar pattern centered around more accurate parameter estimates. The full
                per-parameter breakdown (<a href="#appendix-method-comparison">Figure
                A1</a>) shows these instability patterns in detail.
            </p>

            <p>
                L-BFGS-B with 2D variable projection can match VPNLS precision, but the
                optimizer fails to converge in a non-trivial fraction of scenarios even in this
                relatively small test suite. The choice of finite-difference scheme matters
                considerably. By default, scipy's L-BFGS-B approximates gradients with forward
                differences: each partial derivative is estimated as
                \((f(x + h) - f(x)) / h\). Passing
                <code>jac='3-point'</code> to <code>scipy.optimize.minimize</code> switches to
                3-point central differences, where each partial is estimated as
                \((f(x + h) - f(x - h)) / 2h\). The central formula is generally more
                accurate for smooth objectives because it samples symmetrically around the
                point of interest. In our tests, this closes the precision gap with
                Nelder-Mead (from roughly 10<sup>&minus;5</sup>% to
                10<sup>&minus;8</sup>% error), but introduces sporadic line search failures.
                Notably, these failures can be false positives. The optimizer has already
                reached the true minimum, with residual sum of squares near machine zero, but
                the line search cannot verify further progress because function values are too
                small to distinguish. In scipy, this surfaces as
                <code>result.success = False</code> with an <code>ABNORMAL</code> status
                from <code>scipy.optimize.minimize</code>, even though the returned parameters
                are correct.
            </p>

            <p>
                L-BFGS-B remains a viable alternative to Nelder-Mead for practitioners willing
                to tune settings carefully and who understand that certain convergence errors
                from libraries like scipy may not necessarily be problematic. That said, VPNLS with Nelder-Mead
                is simpler, requires less tuning, and recovers parameter estimates with precision
                at least as high as any other method tested. It technically achieves the most
                precise estimates, though the margin over a well-configured L-BFGS-B with
                3-point central differences is small.
            </p>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    On noise-free synthetic data, VPNLS eliminates the biases inherent in the
                    parabolic approximation and avoids the fragile gradient tuning that
                    complicates L-BFGS-B when used with variable projection. All five loss
                    surface parameters \((E, A, B, \alpha, \beta)\) are recovered with machine
                    precision, and extrapolation to higher compute budgets is exact.
                </p>
            </div>

            <h3 id="method-comparison-inference">Method Comparison (Exponent Inference)</h3>

            <p>
                The parameter recovery results above are noise-free, which is obviously not
                representative of practice. We now extend the
                <a href="#putting-it-together">compounding errors</a> scenario
                (Asymmetric surface with a 3&times; drift at the &plusmn;8&times; grid) to
                a statistical setting. Gaussian noise is added to loss values at three levels
                (&sigma;&nbsp;=&nbsp;0.05,&nbsp;0.1,&nbsp;0.2), the number of compute
                budgets varies from 2 to 4, and the number of points per IsoFLOP curve
                ranges from 4 to 32. Each configuration is repeated with 256 independent
                noise realizations, yielding 9,216 fits per method and 36,864 fits in total.
                <a href="#appendix-noisy-isoflop">Figure&nbsp;A3</a> shows what the noisy
                IsoFLOP samples look like at each noise level.
            </p>

            <p>
                Because a scaling law study is typically run once rather than repeated
                hundreds of times, sporadic optimizer failures that produce large errors in
                a minority of fits are arguably the most consequential practical risk. The
                comparison below emphasizes maximum errors alongside typical accuracy for
                this reason.
            </p>

            <p>
                The <a href="#method-comparison-recovery">parameter recovery comparison</a>
                evaluated all five surface parameters, but Approach&nbsp;2 does not estimate
                them individually. Here we focus on the scaling exponents
                \(a = \beta / (\alpha + \beta)\) and
                \(b = \alpha / (\alpha + \beta)\), which determine compute-optimal
                allocation and are the quantities all four methods can be compared on
                directly. Figure&nbsp;9 pools these exponent errors across all experimental
                conditions.
            </p>

            <figure>
                <img src="figures/exponent_inference/exponent_inference.png"
                    alt="Method comparison showing geometric mean error and max exponent error across four fitting methods on noisy data">
                <figcaption>
                    <strong>Figure 9:</strong> Comparison of four fitting methods on noisy
                    synthetic data, pooled across three noise levels, three budget counts,
                    four points-per-curve settings, and 256 noise realizations (9,216 fits
                    per method). Left: geometric mean of |relative error| (%) with
                    min-to-max range bars, rug ticks for individual errors, and kernel
                    density estimate. Right: maximum |relative error| (%) per exponent.
                    Methods sorted by worst-case error, worst at top.
                    Surface: Asymmetric (&alpha;=0.465, &beta;=0.155) with 3&times; drift,
                    &plusmn;8&times; grid, compute budgets
                    10<sup>17</sup>&ndash;10<sup>21</sup>&nbsp;FLOPs.
                </figcaption>
            </figure>

            <details class="data-table-container">
                <summary>üìä View method comparison data</summary>
                <div class="data-table-wrapper">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Method</th>
                                <th>GMean err%</th>
                                <th>Max a err%</th>
                                <th>Max b err%</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">VPNLS</td>
                                <td>1.09</td>
                                <td>34.2</td>
                                <td>11.4</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">Approach 3 (grid init)</td>
                                <td>1.11</td>
                                <td>73.6</td>
                                <td>24.5</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">Approach 2</td>
                                <td>4.84</td>
                                <td>715.5</td>
                                <td>238.5</td>
                            </tr>
                            <tr>
                                <td style="font-family: Inter, sans-serif;">Approach 3 (random init)</td>
                                <td>16.2</td>
                                <td>296.0</td>
                                <td>98.7</td>
                            </tr>
                        </tbody>
                    </table>
                    <p style="margin-top: 0.75rem; font-size: 0.8rem; color: var(--text-muted);">
                        Geometric mean and maximum |relative error| (%) on scaling exponents
                        \(a\) and \(b\), pooled across 3 noise levels &times; 3 budget counts
                        &times; 4 points-per-curve settings &times; 256 seeds = 9,216 fits per
                        method. Nearly all fits produced usable estimates (one hard failure
                        across all 36,864 fits, in Approach&nbsp;2), though a fraction triggered
                        soft optimizer status flags (iteration limits or abnormal convergence)
                        that did not prevent the optimizer from returning parameter estimates.
                    </p>
                </div>
            </details>

            <p>
                Randomly initialized Approach&nbsp;3 performs the worst overall, with a
                geometric mean error of 16.2% and a maximum error on the \(a\) exponent
                exceeding 290%. This confirms the sensitivity to initialization
                <a href="#approach-3-problems">documented above</a> and serves as a
                reference for what
                unconstrained 5D optimization looks like without careful seeding.
            </p>

            <p>
                Approach&nbsp;2 produces tighter distributions than randomly initialized
                Approach&nbsp;3, but its geometric mean error of 4.8% is more than four
                times higher than VPNLS. Unlike the optimizer-related failures that affect
                Approach&nbsp;3, this error reflects the structural bias from the
                <a href="#asymmetric">parabolic approximation</a>. Even with 32 points
                per curve and low
                noise, the systematic inaccuracy persists. The maximum error on the \(a\)
                exponent exceeds 700%, driven by configurations with few points per curve
                where the parabolic fit to each IsoFLOP curve is less constrained
                (<a href="#appendix-exponent-errors">Figure&nbsp;A4</a>).
            </p>

            <p>
                Grid-initialized Approach&nbsp;3 is substantially more accurate, with a
                geometric mean error of 1.1%. As in the
                <a href="#method-comparison-recovery">parameter recovery comparison</a>,
                both methods use equal-sized initialization grids
                (4<sup>5</sup>&nbsp;=&nbsp;1,024 for Approach&nbsp;3 and
                32<sup>2</sup>&nbsp;=&nbsp;1,024 for VPNLS), so any accuracy difference
                reflects the optimizer rather than an initialization advantage. Despite
                similar typical accuracy, Approach&nbsp;3's maximum error on the \(a\)
                exponent reaches 73.6%, compared to 34.2% for VPNLS.
            </p>

            <p>
                The gap between grid-initialized Approach&nbsp;3 and VPNLS is most visible
                in the tails of the error distribution. Both methods achieve nearly
                identical typical accuracy (geometric mean of 1.1% vs 1.1%), but
                Approach&nbsp;3's eight largest errors range from 66% to 74%, whereas VPNLS's eight largest all fall below 35%. The
                detailed breakdown in
                <a href="#appendix-exponent-errors">Figure&nbsp;A4</a> shows that these
                sporadic Approach&nbsp;3 failures appear across different noise levels,
                budget counts, and dataset sizes without a clear pattern that would
                help anticipate them.
            </p>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    On asymmetric loss surfaces with realistic noise, Approach&nbsp;2's
                    structural bias persists. Its geometric mean exponent error (4.8%) is
                    more than four times that of VPNLS (1.1%), and its worst-case error on
                    the \(a\) exponent exceeds 700%. Approach&nbsp;3 without careful
                    initialization fares even worse, with a geometric mean of 16.2% and
                    worst-case errors approaching 300%. With grid initialization,
                    Approach&nbsp;3 matches VPNLS in typical accuracy but produces
                    sporadic large errors (max 73.6% vs 34.2% on \(a\)) that appear
                    unpredictably across IsoFLOP experiment design conditions. VPNLS offers the most reliable
                    accuracy overall.
                </p>
            </div>

        </section>

        <!-- Section 7: Conclusion -->
        <section id="conclusion">
            <h2>Conclusion</h2>

            <p>
                The Approach&nbsp;2 biases documented in this article are structural, not
                statistical. They exist on noise-free data with perfect experimental
                conditions and, as the
                <a href="#method-comparison-inference">noisy method comparison</a>
                confirms, persist under realistic noise levels with varying amounts
                of data.
            </p>

            <p>
                Two independent sources of error compound in practice. Surface asymmetry
                (\(\alpha \neq \beta\)) biases intercepts, and off-center sampling biases
                intercepts or exponents depending on whether the offset is constant or
                varies with compute budget. Both act simultaneously in any real experiment,
                and published scaling law studies frequently show clear signs of
                off-center sampling and report high levels of asymmetry, at least
                among those that publish IsoFLOP curves. At practical grid widths
                with Chinchilla-like asymmetry, token count errors of 5% or more are
                typical; on more asymmetric surfaces, the errors reach 20% or more.
            </p>

            <p>
                A practical alternative exists. VPNLS (Variable Projection with
                Non-negative Least Squares) recovers all five surface parameters with
                machine precision on noise-free data. Under noise, VPNLS achieves
                typical accuracy on scaling exponents comparable to well-initialized
                Approach&nbsp;3 while producing tighter worst-case errors. It uses the
                same intuitive linear separation that makes Approach 2 appealing and is
                straightforward to implement.
            </p>

            <p>
                Because VPNLS recovers the full loss surface rather than just scaling
                exponents, it may also provide a more precise foundation for the
                analytical extensions to the Chinchilla model discussed in the
                <a href="#motivation">introduction</a>. These extensions build on the same functional form
                and in most cases retain the partially linear structure that variable
                projection exploits, making them a natural direction for future work.
            </p>

            <p>
                Practitioners using Approach 2 should be aware that intercept estimates
                carry a systematic bias that grows with exponent asymmetry and sampling
                grid width. This bias persists under noise: even with generous data and
                low noise levels, Approach&nbsp;2's exponent errors remain several times
                larger than those of methods that fit the full surface. When precision
                matters for extrapolation to large compute
                budgets, VPNLS offers one robust alternative, though the underlying
                principle is more general. Any method that exploits the linear
                separability of the Chinchilla loss surface can avoid the parabolic
                approximation while retaining much of Approach 2's simplicity.
            </p>

            <h3 id="limitations">Limitations</h3>

            <p>
                Several limitations scope the conclusions of this study. We highlight
                the most important ones here.
            </p>

            <ul>
                <li>
                    <strong>Irreducible loss dominance at large scale.</strong>
                    At sufficiently large compute budgets, scaling properties are dominated
                    entirely by the irreducible loss \(E\). When token counts and model sizes
                    at fixed compute budgets are large enough, the Chinchilla surface reaches
                    \(E\) asymptotically and all training configurations become equally
                    effective, meaning that extrapolations are irrelevant and compute-optimal
                    training is no longer informed by scaling laws. We assume this study is
                    only relevant to practitioners working in a regime where downstream model
                    quality can still effectively be informed by scaling law extrapolations
                    per the Chinchilla model.
                </li>
                <li>
                    <strong>No quantification of downstream cost.</strong>
                    We do not connect token extrapolation error to under- or over-training,
                    model performance, or the ultimate cost of Approach 2's errors in FLOPs
                    or dollars. We avoid this because it is difficult to do well, alternatives
                    to Approach 2 can be justified by theory and simulation alone, and those
                    alternatives are easy to implement at effectively no extra computational
                    cost.
                </li>
                <li>
                    <strong>Assumed correctness of the Chinchilla loss surface.</strong>
                    We assume the Chinchilla loss surface model
                    \(L(N, D) = E + A/N^\alpha + B/D^\beta\) is correct in practice. While
                    there is substantial evidence legitimizing this
                    model<sup><a href="#ref-chinchilla_robustness">[15]</a></sup>,
                    alternatives exist, including the Kaplan loss
                    model<sup><a href="#ref-kaplan_scaling">[16]</a></sup>,
                    refined analytical surfaces like
                    Farseer<sup><a href="#ref-farseer">[26]</a></sup> and
                    MuPT<sup><a href="#ref-mupt">[18]</a></sup>, and agent-discovered
                    functional forms<sup><a href="#ref-sld_agent">[27]</a></sup>.
                </li>
                <li>
                    <strong>Qualitative characterization of published study errors.</strong>
                    Likely errors in published studies are characterized qualitatively rather
                    than quantified. We believe the qualitative characterization is compelling
                    enough on its own to justify that real IsoFLOP sampling pathologies occur
                    in practice, but they are difficult to quantify precisely because they do
                    not follow the convenient theoretical model we use for those pathologies
                    in our simulations.
                </li>
            </ul>

        </section>

        <!-- Appendix -->
        <section id="appendix">
            <h2>Appendix</h2>

            <h3 id="appendix-method-comparison">A. Detailed Method Comparison</h3>

            <figure>
                <img src="appendix/parameter_recovery_detailed.png"
                    alt="Detailed method comparison showing per-parameter error across surfaces and sampling ranges">
                <figcaption>
                    <strong>Figure A1:</strong> Per-parameter recovery error for nine fitting
                    methods across three loss surfaces and 20 sampling ranges (baseline, no
                    bias). Each panel shows absolute relative error (%) on a log scale versus
                    sampling range, with one curve per method. Rows correspond to loss surfaces
                    (symmetric, Chinchilla, high imbalance); columns correspond to parameters
                    (E, A, B, &alpha;, &beta;). Gaps indicate convergence failures.
                </figcaption>
            </figure>

            <h3 id="appendix-combined-extrapolation">B. Combined Extrapolation Error by Compute Budget</h3>

            <figure>
                <img src="appendix/combined_extrapolation_error.png"
                    alt="Extrapolation error in D* across compute budgets for all surfaces, sampling ranges, and bias configurations">
                <figcaption>
                    <strong>Figure A2:</strong> Relative error in compute-optimal token count
                    \(D^*\) when extrapolating from the fitting range (10<sup>17</sup>&ndash;10<sup>21</sup>
                    FLOPs) to higher compute budgets (10<sup>22</sup>&ndash;10<sup>25</sup> FLOPs),
                    with asymmetry and sampling biases acting simultaneously. Columns correspond
                    to loss surfaces (symmetric, Chinchilla, Asymmetric); rows correspond to
                    sampling ranges (narrow ¬±2√ó, medium ¬±16√ó, wide ¬±100√ó). The wide row uses
                    an extreme grid width not employed in the main text, included here to
                    further illustrate how far results can deviate with a misconfigured
                    experiment. Each curve represents a different sampling bias configuration:
                    baseline (no bias), two linear drift rates (drift_0.2 and drift_0.4,
                    where the value is the log<sub>10</sub> offset at the highest compute
                    budget), and two constant center offsets (scale_1.5 and scale_2.0, where
                    the value is the multiplicative factor applied at every budget). On
                    symmetric surfaces, errors are driven entirely by off-center sampling;
                    on asymmetric surfaces, the inherent surface bias adds a constant offset
                    visible as the non-zero baseline curve. Drift-based biases produce errors
                    that grow with extrapolation distance (steeper curves), while constant
                    offsets and surface asymmetry produce flat or slowly varying errors. With
                    wider sampling ranges, surface asymmetry dominates and can either reinforce
                    or partially offset the sampling biases.
                </figcaption>
            </figure>

            <h3 id="appendix-noisy-isoflop">C. IsoFLOP Samples with Noise</h3>

            <figure>
                <img src="appendix/isoflop_curves_noisy.png"
                    alt="IsoFLOP samples at three noise levels showing noisy data points, true surface, and drifting sampling centers">
                <figcaption>
                    <strong>Figure A3:</strong> Noisy IsoFLOP samples used in the exponent
                    inference comparison (<a href="#method-comparison-inference">Figure&nbsp;9</a>).
                    Columns correspond to noise levels
                    (&sigma;&nbsp;=&nbsp;0.05,&nbsp;0.1,&nbsp;0.2); rows show loss versus
                    log<sub>10</sub>(N) (top) and log<sub>10</sub>(D) (bottom). Scatter points
                    are noisy observations; solid curves show the noiseless reference surface.
                    Red &times; marks the true compute-optimal point at each budget; black
                    diamonds mark the sampling centers, which drift away from the true optima
                    at higher compute budgets.
                    Surface: Asymmetric (&alpha;=0.465, &beta;=0.155), 4 budgets
                    (10<sup>17</sup>&ndash;10<sup>21</sup>&nbsp;FLOPs), 32 points per curve,
                    &plusmn;8&times; grid.
                </figcaption>
            </figure>

            <h3 id="appendix-exponent-errors">D. Exponent Inference Error Breakdown</h3>

            <figure>
                <img src="appendix/exponent_inference_errors.png"
                    alt="Boxplots of exponent inference errors broken down by noise level, budget count, and points per IsoFLOP curve">
                <figcaption>
                    <strong>Figure A4:</strong> Per-condition breakdown of exponent inference
                    errors from <a href="#method-comparison-inference">Figure&nbsp;9</a>.
                    Rows correspond to number of compute budgets (2, 3, 4); left columns show
                    the lowest noise level (&sigma;=0.05) and right columns show the highest
                    (&sigma;=0.2), each split by exponent (\(a\) and \(b\)). Within each
                    panel, boxplots show absolute relative error (%) on a log scale for each
                    method at each points-per-curve setting (4, 8, 16, 32), over 256 noise
                    realizations. Approach&nbsp;3's sporadic large errors (outlier points above
                    the upper whiskers) appear across conditions without concentrating in any
                    single noise level, budget count, or dataset size.
                </figcaption>
            </figure>
        </section>

        <!-- References -->
        <section id="references">
            <h2>References</h2>
            <ol>
            <li id="ref-chinchilla">"Training Compute-Optimal Large Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a></li>
            <li id="ref-llama3">"The Llama 3 Herd of Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2407.21783">https://arxiv.org/abs/2407.21783</a></li>
            <li id="ref-deepseek">"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2401.02954">https://arxiv.org/abs/2401.02954</a></li>
            <li id="ref-ehr_scaling">"Exploring Scaling Laws for EHR Foundation Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2505.22964">https://arxiv.org/abs/2505.22964</a></li>
            <li id="ref-evo">"Sequence modeling and design from molecular to genome scale with Evo," <em>bioRxiv</em>. <a href="https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2">https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2</a></li>
            <li id="ref-il_scaling">"Scaling Laws for Imitation Learning in Single-Agent Games," <em>TMLR</em>. <a href="https://arxiv.org/abs/2307.09423">https://arxiv.org/abs/2307.09423</a></li>
            <li id="ref-sovit">"Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design," <em>NeurIPS</em>. <a href="https://arxiv.org/abs/2305.13035">https://arxiv.org/abs/2305.13035</a></li>
            <li id="ref-waymo_scaling">"Scaling Laws of Motion Forecasting and Planning -- Technical Report," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2506.08228">https://arxiv.org/abs/2506.08228</a></li>
            <li id="ref-optibert">"Training compute-optimal transformer encoder models," <em>Other</em>. <a href="https://aclanthology.org/2025.emnlp-main.1804.pdf">https://aclanthology.org/2025.emnlp-main.1804.pdf</a></li>
            <li id="ref-dit_scaling">"Scaling Laws For Diffusion Transformers," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2410.08184">https://arxiv.org/abs/2410.08184</a></li>
            <li id="ref-dlm_scaling">"Scaling Behavior of Discrete Diffusion Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2512.10858">https://arxiv.org/abs/2512.10858</a></li>
            <li id="ref-biosignal_scaling">"Scaling Laws for Compute Optimal Biosignal Transformers," <em>Other</em>. <a href="https://dspacemainprd01.lib.uwaterloo.ca/server/api/core/bitstreams/b66b1078-b359-4688-8dac-45e78806eb3d/content">https://dspacemainprd01.lib.uwaterloo.ca/server/api/core/bitstreams/b66b1078-b359-4688-8dac-45e78806eb3d/content</a></li>
            <li id="ref-misfitting">"(Mis)fitting: A Survey of Scaling Laws," <em>ICLR 2025</em>. <a href="https://arxiv.org/abs/2502.18969">https://arxiv.org/abs/2502.18969</a></li>
            <li id="ref-data_filtering_scaling">"Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic," <em>CVPR 2024</em>. <a href="https://arxiv.org/abs/2404.07177">https://arxiv.org/abs/2404.07177</a></li>
            <li id="ref-chinchilla_robustness">"Evaluating the Robustness of Chinchilla Compute-Optimal Scaling," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2509.23963">https://arxiv.org/abs/2509.23963</a></li>
            <li id="ref-kaplan_scaling">"Scaling Laws for Neural Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a></li>
            <li id="ref-data_constrained">"Scaling Data-Constrained Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2305.16264">https://arxiv.org/abs/2305.16264</a></li>
            <li id="ref-mupt">"MuPT: A Generative Symbolic Music Pretrained Transformer," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2404.06393">https://arxiv.org/abs/2404.06393</a></li>
            <li id="ref-precision_scaling">"Scaling Laws for Precision," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2411.04330">https://arxiv.org/abs/2411.04330</a></li>
            <li id="ref-reconciling_scaling">"Reconciling Kaplan and Chinchilla Scaling Laws," <em>TMLR</em>. <a href="https://arxiv.org/abs/2406.12907">https://arxiv.org/abs/2406.12907</a></li>
            <li id="ref-quality_scaling">"Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2510.03313">https://arxiv.org/abs/2510.03313</a></li>
            <li id="ref-optimal_data_mixtures">"Scaling Laws for Optimal Data Mixtures," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2507.09404">https://arxiv.org/abs/2507.09404</a></li>
            <li id="ref-redundancy_scaling">"Scaling Laws are Redundancy Laws," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2509.20721">https://arxiv.org/abs/2509.20721</a></li>
            <li id="ref-moe_scaling">"Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2507.17702">https://arxiv.org/abs/2507.17702</a></li>
            <li id="ref-ai2_task_scaling">"Establishing Task Scaling Laws via Compute-Efficient Model Ladders," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2412.04403">https://arxiv.org/abs/2412.04403</a></li>
            <li id="ref-farseer">"Predictable Scale: Part II, Farseer: A Refined Scaling Law in Large Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2506.10972">https://arxiv.org/abs/2506.10972</a></li>
            <li id="ref-sld_agent">"Can Language Models Discover Scaling Laws?," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2507.21184">https://arxiv.org/abs/2507.21184</a></li>
            <li id="ref-hessian_optimization">"An Investigation into Neural Net Optimization via Hessian Eigenvalue Density," <em>ArXiv</em>. <a href="https://arxiv.org/abs/1901.10159">https://arxiv.org/abs/1901.10159</a></li>
            </ol>
        </section>

    </article>

    <!-- Prevent line breaks between inline math and trailing punctuation -->
    <script>
        document.getElementById('MathJax-script').addEventListener('load', () => {
            MathJax.startup.promise.then(() => {
                document.querySelectorAll('mjx-container.MathJax:not([display="true"])').forEach(el => {
                    const next = el.nextSibling;
                    if (next && next.nodeType === 3) {
                        const match = next.textContent.match(/^([.,;:!?)]+)/);
                        if (match) {
                            const wrapper = document.createElement('span');
                            wrapper.style.whiteSpace = 'nowrap';
                            el.parentNode.insertBefore(wrapper, el);
                            wrapper.appendChild(el);
                            const punct = next.textContent.slice(0, match[1].length);
                            next.textContent = next.textContent.slice(match[1].length);
                            wrapper.appendChild(document.createTextNode(punct));
                        }
                    }
                });
            });
        });
    </script>
</body>

</html>