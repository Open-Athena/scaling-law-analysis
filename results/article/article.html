<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Problems with Chinchilla Approach 2</title>
    <meta name="description"
        content="An analysis of systematic biases in scaling law inference using the IsoFLOP parabola fits method.">

    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap"
        rel="stylesheet">

    <style>
        :root {
            --bg-primary: #ffffff;
            --bg-secondary: #f9fafb;
            --bg-tertiary: #f3f4f6;
            --text-primary: #191919;
            --text-secondary: #525252;
            --text-muted: #8b8b8b;
            --accent-primary: #c4704f;
            --accent-secondary: #d4a574;
            --accent-success: #16a34a;
            --accent-error: #dc2626;
            --border-color: #e5e5e5;
            --code-bg: #f5f5f5;
            --code-text: #c4704f;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.75;
            font-size: 17px;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        .container {
            max-width: 920px;
            margin: 0 auto;
            padding: 5rem 2rem;
        }

        /* Header */
        header {
            margin-bottom: 3.5rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--border-color);
        }

        h1 {
            font-size: 2.75rem;
            font-weight: 600;
            line-height: 1.15;
            margin-bottom: 1.25rem;
            color: var(--text-primary);
            letter-spacing: -0.02em;
        }

        .subtitle {
            font-size: 1.25rem;
            color: var(--text-secondary);
            font-weight: 400;
            line-height: 1.5;
        }

        /* Sections */
        section {
            margin-bottom: 4rem;
        }

        h2 {
            font-size: 1.75rem;
            font-weight: 600;
            margin-bottom: 1.5rem;
            color: var(--text-primary);
            padding-top: 1rem;
            letter-spacing: -0.01em;
        }

        h3 {
            font-size: 1.25rem;
            font-weight: 600;
            margin: 2.5rem 0 1rem;
            color: var(--text-primary);
        }

        h4 {
            font-size: 1.1rem;
            font-weight: 600;
            margin: 2rem 0 1rem;
            color: var(--text-primary);
        }

        p {
            margin-bottom: 1.5rem;
            color: var(--text-primary);
        }

        a {
            color: var(--accent-primary);
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s ease;
        }

        a:hover {
            border-bottom-color: var(--accent-primary);
        }

        /* Lists */
        ul,
        ol {
            margin-bottom: 1.5rem;
            padding-left: 1.5rem;
        }

        li {
            margin-bottom: 0.6rem;
            color: var(--text-primary);
        }

        /* Emphasis */
        strong {
            font-weight: 600;
            color: var(--text-primary);
        }

        em {
            font-style: italic;
        }

        /* Code */
        code {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.875em;
            background: var(--code-bg);
            padding: 0.2em 0.45em;
            border-radius: 4px;
            color: var(--code-text);
        }

        /* Math blocks */
        .math-block {
            background: var(--bg-secondary);
            border-left: 3px solid var(--accent-secondary);
            padding: 1.5rem 1.75rem;
            margin: 2rem 0;
            border-radius: 0 6px 6px 0;
            overflow-x: auto;
        }

        /* Figures */
        figure {
            margin: 2.5rem 0;
            text-align: center;
        }

        figure img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08), 0 4px 12px rgba(0, 0, 0, 0.05);
        }

        figcaption {
            margin-top: 1rem;
            font-size: 0.9rem;
            color: var(--text-secondary);
            text-align: left;
            line-height: 1.6;
        }

        figcaption strong {
            color: var(--text-primary);
        }

        /* Tables */
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
            font-size: 0.95rem;
        }

        .comparison-table th,
        .comparison-table td {
            padding: 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        .comparison-table th {
            background: var(--bg-secondary);
            font-weight: 600;
            color: var(--text-primary);
            font-size: 0.85rem;
            text-transform: uppercase;
            letter-spacing: 0.03em;
        }

        .comparison-table td {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.875em;
        }

        .comparison-table tbody tr {
            transition: background 0.15s ease;
        }

        .comparison-table tbody tr:hover {
            background: var(--bg-tertiary);
        }

        /* Collapsible data tables */
        details.data-table-container {
            margin: 1.5rem 0;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            background: var(--bg-secondary);
        }

        details.data-table-container summary {
            padding: 0.75rem 1.25rem;
            cursor: pointer;
            font-weight: 500;
            color: var(--text-secondary);
            font-size: 0.9rem;
            user-select: none;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        details.data-table-container summary:hover {
            background: var(--bg-tertiary);
            border-radius: 8px;
        }

        details.data-table-container[open] summary {
            border-bottom: 1px solid var(--border-color);
            border-radius: 8px 8px 0 0;
        }

        details.data-table-container .data-table-wrapper {
            padding: 1rem;
            overflow-x: auto;
        }

        .data-table {
            width: 100%;
            border-collapse: collapse;
            font-size: 0.85rem;
        }

        .data-table th,
        .data-table td {
            padding: 0.6rem 0.75rem;
            text-align: right;
            border-bottom: 1px solid var(--border-color);
            white-space: nowrap;
        }

        .data-table th {
            background: var(--bg-tertiary);
            font-weight: 600;
            color: var(--text-primary);
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.03em;
        }

        .data-table td {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.8em;
        }

        .data-table td:first-child,
        .data-table th:first-child {
            text-align: left;
        }

        .data-table tbody tr:hover {
            background: rgba(196, 112, 79, 0.05);
        }

        .data-table td[title] {
            cursor: help;
            border-bottom: 1px dotted var(--text-muted);
        }

        .data-table .error-zero {
            color: var(--accent-success);
            font-weight: 500;
        }

        .data-table .error-low {
            color: #ca8a04;
            font-weight: 600;
        }

        .data-table .error-medium {
            color: #ea580c;
            font-weight: 600;
        }

        .data-table .error-high {
            color: var(--accent-error);
            font-weight: 700;
            background: rgba(220, 38, 38, 0.1);
            border-radius: 3px;
            padding: 0.15em 0.35em;
        }

        .data-table .surface-header {
            font-weight: 600;
            background: var(--bg-tertiary);
            color: var(--text-primary);
        }

        .data-table .surface-header td {
            font-family: 'Inter', sans-serif;
            font-size: 0.85rem;
        }

        /* Error highlighting */
        .error-negligible {
            color: var(--accent-success);
            font-weight: 500;
        }

        .error-significant {
            color: var(--accent-error);
            font-weight: 600;
            background: rgba(220, 38, 38, 0.08);
            padding: 0.15em 0.35em;
            border-radius: 3px;
        }

        /* Callouts */
        .callout {
            background: var(--bg-secondary);
            border-radius: 8px;
            padding: 1.5rem 1.75rem;
            margin: 2rem 0;
            border: 1px solid var(--border-color);
        }

        .callout.key-point {
            border-left: 4px solid var(--accent-primary);
            border-left-width: 4px;
        }

        .callout.warning {
            border-left: 4px solid #d97706;
        }

        .callout-title {
            font-weight: 600;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            color: var(--text-primary);
            font-size: 0.9rem;
        }

        .callout p {
            color: var(--text-secondary);
            font-size: 0.95rem;
        }

        .callout p:last-child {
            margin-bottom: 0;
        }

        /* Process steps */
        .process-steps {
            background: var(--bg-secondary);
            border-radius: 8px;
            padding: 1.5rem 1.5rem 1.5rem 2rem;
            margin: 2rem 0;
            border: 1px solid var(--border-color);
        }

        .process-steps ol {
            margin: 0;
            padding-left: 1.2rem;
        }

        .process-steps li {
            padding: 0.65rem 0;
            border-bottom: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        .process-steps li strong {
            color: var(--text-primary);
        }

        .process-steps li:last-child {
            border-bottom: none;
        }

        /* Responsive */
        @media (max-width: 600px) {
            .container {
                padding: 2.5rem 1.25rem;
            }

            h1 {
                font-size: 2rem;
            }

            h2 {
                font-size: 1.5rem;
            }

            .comparison-table {
                font-size: 0.85rem;
            }

            .comparison-table th,
            .comparison-table td {
                padding: 0.75rem 0.5rem;
            }

            .callout,
            .process-steps,
            .math-block {
                padding: 1.25rem;
            }
        }
    </style>
</head>

<body>
    <article class="container">
        <header>
            <h1>Problems with Chinchilla Approach 2</h1>
            <p class="subtitle">Systematic biases in scaling law inference from IsoFLOP parabola fits</p>
        </header>

        <!-- Section: Motivation -->
        <section id="motivation">
            <h2>Motivation</h2>

            <p>
                <em>[TBD: Origin of Approach 2, the Chinchilla paper context]</em>
            </p>

            <p>
                <em>[TBD: Who uses this method ‚Äî labs, researchers, practitioners applying scaling laws]</em>
            </p>

            <p>
                <em>[TBD: Why they use it ‚Äî simplicity, avoids nonlinear optimization, interpretable steps]</em>
            </p>

            <p>
                <em>[TBD: Why I'm personally using it ‚Äî specific context for this analysis, scaling laws for scientific data modalities]</em>
            </p>

            <p>
                This article examines pitfalls of Approach 2 using noise-free synthetic data.
                By eliminating statistical noise, we isolate the systematic biases inherent
                to the method itself.
            </p>
        </section>

        <!-- Section 1: Approaches to Fitting Scaling Laws -->
        <section id="approaches">
            <h2>Approaches to Fitting Scaling Laws</h2>

            <p>
                Neural scaling laws describe how model performance improves with compute.
                The Chinchilla loss surface models this relationship as:
            </p>

            <div class="math-block">
                \[
                L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}
                \]
            </div>

            <p>
                where \(N\) is the number of parameters, \(D\) is the number of training tokens,
                \(E\) is the irreducible loss, and \(A, B, \alpha, \beta\) capture how quickly
                performance improves with scale.
            </p>

            <p>
                Given a compute budget \(C \approx 6ND\), the optimal allocation satisfies:
            </p>

            <div class="math-block">
                \[
                N^* \propto C^a \quad \text{where} \quad a = \frac{\beta}{\alpha + \beta}
                \]
                \[
                D^* \propto C^b \quad \text{where} \quad b = \frac{\alpha}{\alpha + \beta}
                \]
            </div>

            <p>
                Recovering the exponents \(a\) and \(b\) from empirical training runs is crucial
                for planning efficient large-scale training. Two canonical approaches exist:
            </p>

            <h3>Approach 2: IsoFLOP Parabolic Fitting</h3>

            <p>
                This method is presented in the Chinchilla paper. The key insight is that
                along a fixed-compute contour (IsoFLOP curve), loss as a function of \(\log N\)
                is approximately parabolic near the optimum.
            </p>

            <div class="process-steps">
                <ol>
                    <li>
                        <strong>Sample IsoFLOP contours:</strong> For each compute budget \(C\),
                        train models at various \((N, D)\) pairs satisfying \(C = 6ND\)
                    </li>
                    <li>
                        <strong>Fit parabolas:</strong> For each budget, fit
                        \(L = a(\log N)^2 + b(\log N) + c\) and extract the minimum \(N^*\)
                    </li>
                    <li>
                        <strong>Fit power laws:</strong> Regress \(\log N^*\) against \(\log C\)
                        to recover the exponent \(a\) (and similarly for \(D^*\), \(b\))
                    </li>
                </ol>
            </div>

            <p>
                The appeal is simplicity: only polynomial fits, no nonlinear optimization.
                The parabolic approximation comes from a Taylor expansion of the loss surface
                around the optimum.
            </p>

            <h3>Approach 3: Direct Surface Fitting</h3>

            <p>
                The alternative is to fit all five parameters \((E, A, B, \alpha, \beta)\)
                simultaneously via nonlinear least squares. This avoids the parabolic
                approximation entirely but is notoriously unstable: highly sensitive to
                initialization and prone to converging to spurious local minima.
            </p>
        </section>

        <!-- Section 2: The Happy Path -->
        <section id="happy-path">
            <h2>The Happy Path ‚Äî Symmetric Surfaces</h2>

            <p>
                Before examining failure modes, let's establish that Approach 2 works
                perfectly under ideal conditions. Consider a <strong>symmetric</strong>
                loss surface where \(\alpha = \beta\):
            </p>

            <div class="math-block">
                \[
                L(N, D) = 1.69 + \frac{400}{N^{0.31}} + \frac{400}{D^{0.31}}
                \]
            </div>

            <p>
                With equal exponents, the optimal allocation splits compute evenly between
                parameters and data. The true scaling exponents are:
            </p>

            <div class="math-block">
                \[
                a = b = \frac{0.31}{0.31 + 0.31} = 0.5
                \]
            </div>

            <p>
                We sample five IsoFLOP contours spanning \(10^{17}\) to \(10^{21}\) FLOPs,
                fit parabolas to each, and extract the optimal token count \(D^*\).
            </p>

            <figure>
                <img src="happy_path.png" alt="Approach 2 on symmetric surface showing perfect recovery">
                <figcaption>
                    <strong>Figure 1:</strong> Approach 2 applied to a symmetric loss surface.
                    Left: IsoFLOP curves with fitted parabolas. True (√ó) and inferred (+) optima
                    are indistinguishable. Right: Power-law fit recovers the exact scaling exponent.
                </figcaption>
            </figure>

            <p>
                The results confirm perfect recovery of the token scaling exponent and intercept:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>True Value</th>
                        <th>Inferred Value</th>
                        <th>Relative Error</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>b (D* exponent)</td>
                        <td>0.500000</td>
                        <td>0.500000</td>
                        <td>+6.2√ó10‚Åª¬π¬≤%</td>
                    </tr>
                    <tr>
                        <td>b‚ÇÄ (D* intercept)</td>
                        <td>‚àí0.389076</td>
                        <td>‚àí0.389076</td>
                        <td>‚àí1.4√ó10‚Åª¬π‚Å∞%</td>
                    </tr>
                </tbody>
            </table>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    On a symmetric loss surface with perfectly crafted IsoFLOP grid sampling, 
                    Approach 2 recovers both exponents and intercepts with machine-precision 
                    accuracy. The parabolic approximation is exact when \(\alpha = \beta\).
                </p>
            </div>

            <p>
                This establishes our baseline: Approach 2 is precisely correct under ideal conditions that 
                are unrealistic in practice.  The problems arise when we deviate from these ideal conditions, as we'll see
                in the following sections where these conditions are perturbed in controlled ways.
            </p>
        </section>

        <!-- Section 3: Asymmetric Surfaces -->
        <section id="asymmetric">
            <h2>Asymmetric Surfaces ‚Äî Intercept and Extrapolation Errors</h2>

            <p>
                We repeat the exact same procedure as before: perfect sampling centers, no noise,
                identical methodology. The only change is that the loss surface is now <strong>asymmetric</strong>
                (\(\alpha \neq \beta\)).
            </p>

            <h3>What Happens</h3>

            <p>
                Simulation results show that when the loss surface is asymmetric, Approach 2 produces
                systematically wrong intercepts while exponents remain accurate. This isn't statistical
                noise; it's a deterministic bias from fitting parabolas to a non-parabolic surface.
            </p>

            <p>
                We test two configurations to see how the effect scales:
            </p>

            <ul>
                <li><strong>Chinchilla:</strong> \(\alpha = 0.34\), \(\beta = 0.28\) (ratio ‚âà 1.2)</li>
                <li><strong>High Imbalance:</strong> \(\alpha = 0.46\), \(\beta = 0.15\) (ratio = 3.0)</li>
            </ul>

            <figure>
                <img src="asymmetric.png" alt="Approach 2 on asymmetric surfaces showing intercept errors">
                <figcaption>
                    <strong>Figure 2:</strong> Approach 2 on asymmetric loss surfaces.
                    Note the visible gap between true (dashed) and inferred (solid) power-law
                    lines in the High Imbalance case. The exponents match perfectly, but
                    the intercepts differ.
                </figcaption>
            </figure>

            <h4>Chinchilla Surface</h4>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>True Value</th>
                        <th>Inferred Value</th>
                        <th>Relative Error</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>b (D* exponent)</td>
                        <td>0.548387</td>
                        <td>0.548387</td>
                        <td class="error-negligible">‚âà 0%</td>
                    </tr>
                    <tr>
                        <td>b‚ÇÄ (D* intercept)</td>
                        <td>‚àí0.555357</td>
                        <td>‚àí0.578092</td>
                        <td class="error-significant">‚àí4.1%</td>
                    </tr>
                </tbody>
            </table>

            <h4>High Imbalance Surface</h4>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>True Value</th>
                        <th>Inferred Value</th>
                        <th>Relative Error</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>b (D* exponent)</td>
                        <td>0.750000</td>
                        <td>0.750000</td>
                        <td class="error-negligible">‚âà 0%</td>
                    </tr>
                    <tr>
                        <td>b‚ÇÄ (D* intercept)</td>
                        <td>‚àí1.345791</td>
                        <td>‚àí1.459957</td>
                        <td class="error-significant">‚àí8.5%</td>
                    </tr>
                </tbody>
            </table>

            <h3>Why This Is Surprising</h3>

            <p>
                A few percent error in the intercept might seem minor, but consider that this simulation
                gave Approach 2 every advantage. The data is perfect: no measurement noise, with every
                point lying exactly on the true loss surface. The sampling is perfect too, with IsoFLOP
                grids centered precisely at the true optimum (something you wouldn't know how to do in
                practice). And the parameters are standard, taken directly from the Chinchilla paper
                rather than contrived to expose a potentially unrealistic weakness.
            </p>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    Even under these ideal conditions, Approach 2 produces biased intercepts for
                    asymmetric surfaces. The error is systematic, a property of the parabolic
                    approximation, not statistical noise.
                </p>
            </div>

            <h3>Why It Happens</h3>

            <p>
                The IsoFLOP loss curve is not a true parabola; it contains exponential terms.
                When a parabola is fit to this curve, the parabola's minimum (vertex) doesn't
                land exactly at the true optimum. It shifts slightly, and the key insight is that
                this shift depends only on the loss surface shape (\(\alpha\), \(\beta\)) and
                the sampling grid. It does not depend on compute budget. The sampling grid size
                becomes important here: wider grids amplify the mismatch between the true curve
                and its parabolic approximation, increasing the vertex shift.
            </p>

            <p>
                Since the vertex shift is constant across all compute budgets, it biases every
                inferred \(N^*\) by the same multiplicative factor. When fitting
                \(\log N^*\) vs \(\log C\) to extract scaling exponents:
            </p>

            <ul>
                <li>The <strong>slope (exponent)</strong> is unchanged: multiplying all \(N^*\) values
                    by a constant factor adds a constant to \(\log N^*\), which doesn't affect the slope</li>
                <li>The <strong>intercept</strong> absorbs the entire error, biased by exactly
                    that multiplicative factor</li>
            </ul>

            <p>
                <strong>Exact derivation:</strong> The intercept error can be derived analytically
                in closed form. The parabola vertex shifts by \(\delta w\) (in log-space), giving
                an intercept error of:
            </p>

            <div class="math-block">
                \[
                \text{Intercept error} = 10^{\delta w} - 1
                \]
            </div>

            <p>
                where \(\delta w = f(\alpha, \beta, W, n)\) depends only on the surface exponents
                and the sampling grid (width \(W\) in log-space, number of points \(n\) per IsoFLOP curve), not on \(C\), \(E\),
                \(A\), or \(B\). Here \(W\) spans \(10^{-W/2}\) to \(10^{W/2}\) times the optimal \(N^*\),
                so \(W = 2.41\) (the XL grid) means sampling from \(\frac{1}{16}\times\) to \(16\times\) the optimum.
                And \(n = 10\) means 10 model sizes per compute budget. Key properties:
            </p>

            <ul>
                <li>\(\delta w = 0\) when \(\alpha = \beta\) (symmetric surfaces have no error)</li>
                <li>\(\delta w\) grows with \(|\alpha - \beta|\) (more asymmetry ‚Üí more error)</li>
                <li>\(\delta w\) grows with \(W\) (wider sampling range ‚Üí more error)</li>
            </ul>

            <p>
                For example, with the Chinchilla parameters (\(\alpha = 0.34\), \(\beta = 0.28\)):
                the XS grid (\(W = 0.60\)) yields 0.3% intercept error, while the XL grid
                (\(W = 2.41\)) yields 4.1% error.
            </p>

            <p>
                The <a href="scaling_exponent_errors.html">full derivation</a> provides the
                closed-form expression for vertex shift \(\delta w\) as a function of \(\alpha\),
                \(\beta\), \(W\), and \(n\). It also shows how this shift translates directly
                into intercept error, independent of compute budget.
            </p>

            <p>
                <strong>Intuition via Taylor expansion:</strong> A parabola is a 2nd-order polynomial,
                which is equivalent to a 2nd-order Taylor expansion around the optimum. The approximation
                \(L(w) \approx L(0) + \frac{1}{2}L''(0)w^2\) is only valid when higher-order terms are
                negligible, i.e., when samples are close to the true minimum. As sampling range increases,
                3rd and 4th order terms grow. For symmetric surfaces (\(\alpha = \beta\)), odd-order
                terms cancel by symmetry, preserving the vertex location. For asymmetric surfaces,
                they don't cancel, shifting the fitted vertex away from the true optimum.
            </p>

            <h3>Why It Matters</h3>

            <p>
                Extrapolation to higher compute budgets requires both exponents and intercepts to be correct.
                The previous section established that asymmetric loss surfaces produce provably biased
                intercepts even under ideal experimental conditions. Here we quantify what those errors
                mean in practical terms by examining compute-optimal token prediction: given a compute
                budget, how many tokens does the inferred scaling law predict?
            </p>

            <p>
                Up to this point, all analysis has assumed a single fixed sampling grid width.
                We now examine how token prediction error varies with both compute budget and
                sampling grid width. For surfaces with asymmetric exponents, wider sampling grids
                amplify the parabola-fitting mismatch, increasing the constant vertex shift and
                thus the intercept bias. To make this comparison concrete, we first define what
                "wider" and "narrower" mean in quantitative terms.
            </p>

            <p>
                A sampling grid of "¬±kx" means the sampled values (whether
                model sizes or token counts) range from <sup>1</sup>&frasl;<sub>k</sub> to k times the true optimum at
                each compute budget. The total range covered is k¬≤ (the ratio of largest to
                smallest), and the log‚ÇÅ‚ÇÄ of that ratio tells you how many factors of
                10, or "decades," the grid spans end-to-end (e.g. a value of 1.81 means
                the largest sample is 10<sup>1.81</sup> ‚âà 64x the smallest). The table
                below shows the four grid widths used in this analysis:
            </p>

            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>Grid Name</th>
                        <th>¬±kx</th>
                        <th>Sampling Range</th>
                        <th>Total Ratio</th>
                        <th>Decade Span (factors of 10)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Extra Small (XS)</td>
                        <td>¬±2x</td>
                        <td>1/2x to 2x</td>
                        <td>4x</td>
                        <td>0.60</td>
                    </tr>
                    <tr>
                        <td>Small (S)</td>
                        <td>¬±4x</td>
                        <td>1/4x to 4x</td>
                        <td>16x</td>
                        <td>1.20</td>
                    </tr>
                    <tr>
                        <td>Large (L)</td>
                        <td>¬±8x</td>
                        <td>1/8x to 8x</td>
                        <td>64x</td>
                        <td>1.81</td>
                    </tr>
                    <tr>
                        <td>Extra Large (XL)</td>
                        <td>¬±16x</td>
                        <td>1/16x to 16x</td>
                        <td>256x</td>
                        <td>2.41</td>
                    </tr>
                </tbody>
            </table>

            <p>
                In practice, scaling law experiments typically sample across 1 to 2 decades in
                token count, placing the Small and Large grids squarely within the realistic range.
                The Extra Small and Extra Large grids bracket this range on either side, illustrating
                how the biases shrink or grow as the sampling window narrows or widens. The Extra
                Large grid (¬±16x, ~2.4 decades) is the default used in all single-grid
                analyses in the preceding sections.
            </p>

            <figure>
                <img src="extrapolation_error.png" alt="Bar chart showing token prediction error by surface and grid width">
                <figcaption>
                    <strong>Figure 3:</strong> Relative error in compute-optimal token prediction when
                    extrapolating from the training range (10¬π‚Å∑-10¬≤¬π FLOPs) to 10¬≤‚Å¥ FLOPs. Negative values
                    indicate underestimation: the inferred scaling law predicts fewer tokens than optimal.
                    Bars are grouped by sampling grid width. Annotations for the Chinchilla surface
                    show \(D^*\) (true compute-optimal token count) versus \(\hat{D}^*\) (the
                    Approach 2 estimate).
                </figcaption>
            </figure>

            <details class="data-table-container">
                <summary>üìä View raw data</summary>
                <div class="data-table-wrapper">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Surface</th>
                                <th>Œ±</th>
                                <th>Œ≤</th>
                                <th>Grid</th>
                                <th>True D*</th>
                                <th>Inferred D*</th>
                                <th>Abs Error</th>
                                <th>Rel Error</th>
                            </tr>
                        </thead>
                        <tbody>
                            <!-- Symmetric Surface -->
                            <tr class="surface-header">
                                <td colspan="8">Symmetric Surface (Œ± = Œ≤)</td>
                            </tr>
                            <tr>
                                <td>Symmetric</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>XS (¬±2√ó)</td>
                                <td title="4.082482904638630e+11">408.2B</td>
                                <td title="4.082482904640633e+11">408.2B</td>
                                <td title="2.002563476562500e-01">‚âà0</td>
                                <td title="0.000000000049053%" class="error-zero">‚âà0%</td>
                            </tr>
                            <tr>
                                <td>Symmetric</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>S (¬±4√ó)</td>
                                <td title="4.082482904638630e+11">408.2B</td>
                                <td title="4.082482904638362e+11">408.2B</td>
                                <td title="-2.685546875000000e-02">‚âà0</td>
                                <td title="-0.000000000006578%" class="error-zero">‚âà0%</td>
                            </tr>
                            <tr>
                                <td>Symmetric</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>L (¬±8√ó)</td>
                                <td title="4.082482904638630e+11">408.2B</td>
                                <td title="4.082482904638746e+11">408.2B</td>
                                <td title="1.153564453125000e-02">‚âà0</td>
                                <td title="0.000000000002826%" class="error-zero">‚âà0%</td>
                            </tr>
                            <tr>
                                <td>Symmetric</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>XL (¬±16√ó)</td>
                                <td title="4.082482904638630e+11">408.2B</td>
                                <td title="4.082482904640399e+11">408.2B</td>
                                <td title="1.768798828125000e-01">‚âà0</td>
                                <td title="0.000000000043327%" class="error-zero">‚âà0%</td>
                            </tr>
                            <!-- Chinchilla Surface -->
                            <tr class="surface-header">
                                <td colspan="8">Chinchilla Surface (Œ± ‚â† Œ≤)</td>
                            </tr>
                            <tr>
                                <td>Chinchilla</td>
                                <td>0.34</td>
                                <td>0.28</td>
                                <td>XS (¬±2√ó)</td>
                                <td title="4.035834749563681e+12">4.04T</td>
                                <td title="4.022640080071175e+12">4.02T</td>
                                <td title="-1.319466949250537e+10">‚àí13.2B</td>
                                <td title="-0.326937803732719%" class="error-low">‚àí0.33%</td>
                            </tr>
                            <tr>
                                <td>Chinchilla</td>
                                <td>0.34</td>
                                <td>0.28</td>
                                <td>S (¬±4√ó)</td>
                                <td title="4.035834749563681e+12">4.04T</td>
                                <td title="3.983323492914587e+12">3.98T</td>
                                <td title="-5.251125664909375e+10">‚àí52.5B</td>
                                <td title="-1.301125043704300%" class="error-low">‚àí1.30%</td>
                            </tr>
                            <tr>
                                <td>Chinchilla</td>
                                <td>0.34</td>
                                <td>0.28</td>
                                <td>L (¬±8√ó)</td>
                                <td title="4.035834749563681e+12">4.04T</td>
                                <td title="3.918678311395280e+12">3.92T</td>
                                <td title="-1.171564381684004e+11">‚àí117.2B</td>
                                <td title="-2.902904738135434%" class="error-medium">‚àí2.90%</td>
                            </tr>
                            <tr>
                                <td>Chinchilla</td>
                                <td>0.34</td>
                                <td>0.28</td>
                                <td>XL (¬±16√ó)</td>
                                <td title="4.035834749563681e+12">4.04T</td>
                                <td title="3.829997307632958e+12">3.83T</td>
                                <td title="-2.058374419307222e+11">‚àí205.8B</td>
                                <td title="-5.100244551712022%" class="error-medium">‚àí5.10%</td>
                            </tr>
                            <!-- High Imbalance Surface -->
                            <tr class="surface-header">
                                <td colspan="8">High Imbalance Surface (Œ±/Œ≤ = 3)</td>
                            </tr>
                            <tr>
                                <td>High Imbalance</td>
                                <td>0.465</td>
                                <td>0.155</td>
                                <td>XS (¬±2√ó)</td>
                                <td title="4.510334355952566e+16">45.1Q</td>
                                <td title="4.434797634319416e+16">44.3Q</td>
                                <td title="-7.553672163314960e+14">‚àí755.4T</td>
                                <td title="-1.674747716507073%" class="error-low">‚àí1.67%</td>
                            </tr>
                            <tr>
                                <td>High Imbalance</td>
                                <td>0.465</td>
                                <td>0.155</td>
                                <td>S (¬±4√ó)</td>
                                <td title="4.510334355952566e+16">45.1Q</td>
                                <td title="4.217262808974590e+16">42.2Q</td>
                                <td title="-2.930715469779760e+15">‚àí2.9Q</td>
                                <td title="-6.497778742083532%" class="error-medium">‚àí6.50%</td>
                            </tr>
                            <tr>
                                <td>High Imbalance</td>
                                <td>0.465</td>
                                <td>0.155</td>
                                <td>L (¬±8√ó)</td>
                                <td title="4.510334355952566e+16">45.1Q</td>
                                <td title="3.882927967488314e+16">38.8Q</td>
                                <td title="-6.274063884642520e+15">‚àí6.3Q</td>
                                <td title="-13.910418584294648%" class="error-high">‚àí13.91%</td>
                            </tr>
                            <tr>
                                <td>High Imbalance</td>
                                <td>0.465</td>
                                <td>0.155</td>
                                <td>XL (¬±16√ó)</td>
                                <td title="4.510334355952566e+16">45.1Q</td>
                                <td title="3.467711572211202e+16">34.7Q</td>
                                <td title="-1.042622783741363e+16">‚àí10.4Q</td>
                                <td title="-23.116308048545221%" class="error-high">‚àí23.12%</td>
                            </tr>
                        </tbody>
                    </table>
                    <p style="margin-top: 0.75rem; font-size: 0.8rem; color: var(--text-muted);">
                        B = billion, T = trillion, Q = quadrillion. Hover over cells for full-precision values.
                        Training range: 10¬π‚Å∑‚Äì10¬≤¬π FLOPs. Evaluation budget: 10¬≤‚Å¥ FLOPs.
                    </p>
                </div>
            </details>

            <p>
                The key observations from this figure are:
            </p>

            <ul>
                <li><strong>Symmetric surfaces are immune:</strong> When \(\alpha = \beta\), all grid
                    widths produce zero error</li>
                <li><strong>Asymmetric surfaces underestimate:</strong> Negative errors mean the inferred
                    \(D^*\) is smaller than the true \(D^*\). Following these predictions would undertrain
                    the model</li>
                <li><strong>Wider grids amplify error:</strong> Moving from XS (¬±2x) to XL (¬±16x)
                    grids increases error from 0.3% to 5.1% on Chinchilla, and from 1.7% to 23% on High Imbalance</li>
                <li><strong>Asymmetry magnifies everything:</strong> The High Imbalance surface
                    (\(\alpha/\beta = 3\)) shows roughly 4‚Äì5x larger errors than Chinchilla at each grid width</li>
            </ul>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    Consider the Chinchilla surface with the Large grid (¬±8x), a practical sampling
                    range for real experiments. When extrapolating to 10¬≤‚Å¥ FLOPs, the true optimal
                    token count is 4.04 trillion, but Approach 2 predicts only 3.92 trillion: a 2.9%
                    underestimate, or roughly 117 billion fewer tokens than optimal. While 2.9% may
                    seem modest, recall that this simulation uses unrealistically ideal conditions:
                    perfectly centered sampling grids at every compute budget and zero measurement
                    noise. Real experiments, where the true optimum is unknown, data is noisy,
                    and the scaling exponent imbalance may be larger than Chinchilla's modest
                    \(\alpha/\beta \approx 1.2\), can only do worse.
                </p>
            </div>

        </section>

        <!-- Section 4: Off-Center Sampling -->
        <section id="off-center">
            <h2>Off-Center Sampling ‚Äî Exponent and Extrapolation Errors</h2>

            <p>
                The previous sections assumed perfectly centered sampling: at every compute budget,
                the IsoFLOP grid was placed exactly at the true optimum. In practice, you don't know
                \(N^*\) before running the experiment. Sampling centers are guesses, informed by prior
                estimates or heuristics, and they will inevitably be wrong by some amount.
            </p>

            <p>
                This is a distinct source of error from the asymmetry bias examined earlier. Asymmetry
                errors arise from the shape of the loss surface (\(\alpha \neq \beta\)); off-center
                errors arise from where you place the sampling grid. To isolate this new effect, we
                return to the symmetric surface (\(\alpha = \beta = 0.31\)) where asymmetry bias is
                zero by construction.
            </p>

            <h3>Constant Multiplicative Bias</h3>

            <p>
                The simplest form of off-center sampling is a constant multiplicative offset: every
                compute budget's sampling center is shifted by the same factor from the true optimum.
                A "2√ó offset" means each IsoFLOP grid is centered at \(2 \times D^*\) instead of
                \(D^*\), so the grid midpoint consistently sits at twice the true optimal token count.
            </p>

            <p>
                Because this offset is the same at every compute budget, it has a familiar geometric
                effect: each parabola vertex shifts by a constant amount in log-space. This is the
                same mechanism as asymmetry bias. The slope of \(\log D^*\) vs \(\log C\) is
                unaffected (a constant additive shift in log-space doesn't change the slope), so the
                scaling exponent is preserved perfectly. The intercept, however, absorbs the entire
                error.
            </p>

            <figure>
                <img src="off_center_constant_bias.png"
                    alt="Off-center sampling with constant multiplicative bias showing zero exponent error but systematic intercept error">
                <figcaption>
                    <strong>Figure 4:</strong> Effect of a constant 2√ó offset in sampling centers on
                    the symmetric surface. Left: IsoFLOP curves at the Large grid (¬±8√ó), with black
                    diamonds marking the (off-center) sampling center, red √ó the true \(D^*\), and
                    blue + the inferred \(D^*\). Center and right: exponent and intercept errors
                    across grid widths from XS (¬±2√ó) to XL (¬±16√ó), plotted on the same y-axis scale.
                    The exponent is recovered perfectly (flat at zero) while the intercept shows
                    systematic bias that varies with grid width.
                </figcaption>
            </figure>

            <p>
                The pattern in the intercept error panel is worth noting. At narrow grid widths,
                the intercept error is positive (the inferred \(D^*\) overshoots), then crosses zero
                and becomes negative at wider grids. This reflects how the parabola fit responds to
                the interaction between the off-center shift and the sampling range: at different
                grid widths, the asymmetric tails of the true loss curve are weighted differently,
                pulling the fitted vertex in different directions.
            </p>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Result</div>
                <p>
                    A constant multiplicative offset in sampling centers preserves the scaling
                    exponent perfectly but biases the intercept. This is the same mechanism as
                    asymmetry bias: any effect that shifts every parabola vertex by the same amount
                    in log-space will corrupt the intercept while leaving the exponent intact.
                </p>
            </div>

            <h3>Drifting Bias</h3>

            <p>
                <em>[TBD: When the offset grows with compute budget (e.g., prior estimates become
                progressively worse at higher compute), both exponents and intercepts are corrupted.
                This is qualitatively different from constant bias and represents a more severe
                failure mode.]</em>
            </p>

            <div class="callout key-point">
                <div class="callout-title">‚úì Key Message</div>
                <p>
                    Constant bias preserves exponents; any compute-dependent bias pattern distorts
                    them. The distinction matters because exponent errors compound during extrapolation,
                    while intercept errors remain fixed.
                </p>
            </div>

        </section>

    </article>

    <!-- Prevent line breaks between inline math and trailing punctuation -->
    <script>
        document.getElementById('MathJax-script').addEventListener('load', () => {
            MathJax.startup.promise.then(() => {
                document.querySelectorAll('mjx-container.MathJax:not([display="true"])').forEach(el => {
                    const next = el.nextSibling;
                    if (next && next.nodeType === 3) {
                        const match = next.textContent.match(/^([.,;:!?)]+)/);
                        if (match) {
                            const wrapper = document.createElement('span');
                            wrapper.style.whiteSpace = 'nowrap';
                            el.parentNode.insertBefore(wrapper, el);
                            wrapper.appendChild(el);
                            const punct = next.textContent.slice(0, match[1].length);
                            next.textContent = next.textContent.slice(match[1].length);
                            wrapper.appendChild(document.createTextNode(punct));
                        }
                    }
                });
            });
        });
    </script>
</body>

</html>