<section id="references">
    <h2>References</h2>
    <ol>
    <li id="ref-chinchilla">"Training Compute-Optimal Large Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a></li>
    <li id="ref-llama3">"The Llama 3 Herd of Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2407.21783">https://arxiv.org/abs/2407.21783</a></li>
    <li id="ref-deepseek">"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2401.02954">https://arxiv.org/abs/2401.02954</a></li>
    <li id="ref-ehr_scaling">"Exploring Scaling Laws for EHR Foundation Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2505.22964">https://arxiv.org/abs/2505.22964</a></li>
    <li id="ref-evo">"Sequence modeling and design from molecular to genome scale with Evo," <em>bioRxiv</em>. <a href="https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2">https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2</a></li>
    <li id="ref-il_scaling">"Scaling Laws for Imitation Learning in Single-Agent Games," <em>TMLR</em>. <a href="https://arxiv.org/abs/2307.09423">https://arxiv.org/abs/2307.09423</a></li>
    <li id="ref-sovit">"Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design," <em>NeurIPS</em>. <a href="https://arxiv.org/abs/2305.13035">https://arxiv.org/abs/2305.13035</a></li>
    <li id="ref-waymo_scaling">"Scaling Laws of Motion Forecasting and Planning -- Technical Report," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2506.08228">https://arxiv.org/abs/2506.08228</a></li>
    <li id="ref-optibert">"Training compute-optimal transformer encoder models," <em>Other</em>. <a href="https://aclanthology.org/2025.emnlp-main.1804.pdf">https://aclanthology.org/2025.emnlp-main.1804.pdf</a></li>
    <li id="ref-dit_scaling">"Scaling Laws For Diffusion Transformers," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2410.08184">https://arxiv.org/abs/2410.08184</a></li>
    <li id="ref-dlm_scaling">"Scaling Behavior of Discrete Diffusion Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2512.10858">https://arxiv.org/abs/2512.10858</a></li>
    <li id="ref-biosignal_scaling">"Scaling Laws for Compute Optimal Biosignal Transformers," <em>Other</em>. <a href="https://dspacemainprd01.lib.uwaterloo.ca/server/api/core/bitstreams/b66b1078-b359-4688-8dac-45e78806eb3d/content">https://dspacemainprd01.lib.uwaterloo.ca/server/api/core/bitstreams/b66b1078-b359-4688-8dac-45e78806eb3d/content</a></li>
    <li id="ref-misfitting">"(Mis)fitting: A Survey of Scaling Laws," <em>ICLR 2025</em>. <a href="https://arxiv.org/abs/2502.18969">https://arxiv.org/abs/2502.18969</a></li>
    <li id="ref-data_filtering_scaling">"Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic," <em>CVPR 2024</em>. <a href="https://arxiv.org/abs/2404.07177">https://arxiv.org/abs/2404.07177</a></li>
    <li id="ref-chinchilla_robustness">"Evaluating the Robustness of Chinchilla Compute-Optimal Scaling," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2509.23963">https://arxiv.org/abs/2509.23963</a></li>
    <li id="ref-kaplan_scaling">"Scaling Laws for Neural Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a></li>
    <li id="ref-data_constrained">"Scaling Data-Constrained Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2305.16264">https://arxiv.org/abs/2305.16264</a></li>
    <li id="ref-mupt">"MuPT: A Generative Symbolic Music Pretrained Transformer," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2404.06393">https://arxiv.org/abs/2404.06393</a></li>
    <li id="ref-precision_scaling">"Scaling Laws for Precision," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2411.04330">https://arxiv.org/abs/2411.04330</a></li>
    <li id="ref-reconciling_scaling">"Reconciling Kaplan and Chinchilla Scaling Laws," <em>TMLR</em>. <a href="https://arxiv.org/abs/2406.12907">https://arxiv.org/abs/2406.12907</a></li>
    <li id="ref-quality_scaling">"Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2510.03313">https://arxiv.org/abs/2510.03313</a></li>
    <li id="ref-optimal_data_mixtures">"Scaling Laws for Optimal Data Mixtures," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2507.09404">https://arxiv.org/abs/2507.09404</a></li>
    <li id="ref-redundancy_scaling">"Scaling Laws are Redundancy Laws," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2509.20721">https://arxiv.org/abs/2509.20721</a></li>
    <li id="ref-moe_scaling">"Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2507.17702">https://arxiv.org/abs/2507.17702</a></li>
    <li id="ref-ai2_task_scaling">"Establishing Task Scaling Laws via Compute-Efficient Model Ladders," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2412.04403">https://arxiv.org/abs/2412.04403</a></li>
    <li id="ref-farseer">"Predictable Scale: Part II, Farseer: A Refined Scaling Law in Large Language Models," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2506.10972">https://arxiv.org/abs/2506.10972</a></li>
    <li id="ref-sld_agent">"Can Language Models Discover Scaling Laws?," <em>ArXiv</em>. <a href="https://arxiv.org/abs/2507.21184">https://arxiv.org/abs/2507.21184</a></li>
    </ol>
</section>