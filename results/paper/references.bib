@article{chinchilla,
    title={Training Compute-Optimal Large Language Models},
    author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
    journal={arXiv preprint arXiv:2203.15556},
    year={2022}
}

@article{llama3,
    title={The Llama 3 Herd of Models},
    author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and others},
    journal={arXiv preprint arXiv:2407.21783},
    year={2024}
}

@article{deepseek,
    title={{DeepSeek LLM}: Scaling Open-Source Language Models with Longtermism},
    author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and others},
    journal={arXiv preprint arXiv:2401.02954},
    year={2024}
}

@article{ehr_scaling,
    title={Exploring Scaling Laws for {EHR} Foundation Models},
    author={Jiang, Ethan and others},
    journal={arXiv preprint arXiv:2505.22964},
    year={2025}
}

@article{evo,
    title={Sequence Modeling and Design from Molecular to Genome Scale with {Evo}},
    author={Nguyen, Eric and Poli, Michael and Durrant, Matthew G. and others},
    journal={bioRxiv preprint 2024.02.27.582234},
    year={2024}
}

@article{il_scaling,
    title={Scaling Laws for Imitation Learning in Single-Agent Games},
    author={Haldar, Siddhant and Pinto, Lerrel},
    journal={Transactions on Machine Learning Research},
    year={2023}
}

@inproceedings{sovit,
    title={Getting {ViT} in Shape: Scaling Laws for Compute-Optimal Model Design},
    author={Alabdulmohsin, Ibrahim M. and Neyshabur, Behnam and Zhai, Xiaohua},
    booktitle={NeurIPS},
    year={2023}
}

@article{waymo_scaling,
    title={Scaling Laws of Motion Forecasting and Planning -- Technical Report},
    author={Chen, Yilun and others},
    journal={arXiv preprint arXiv:2506.08228},
    year={2025}
}

@inproceedings{optibert,
    title={Training Compute-Optimal Transformer Encoder Models},
    author={Tay, Yi and others},
    booktitle={EMNLP},
    year={2025}
}

@article{dit_scaling,
    title={Scaling Laws For Diffusion Transformers},
    author={Li, Zhengyang and others},
    journal={arXiv preprint arXiv:2410.08184},
    year={2024}
}

@article{dlm_scaling,
    title={Scaling Behavior of Discrete Diffusion Language Models},
    author={Nie, Shen and others},
    journal={arXiv preprint arXiv:2512.10858},
    year={2025}
}

@article{biosignal_scaling,
    title={Scaling Laws for Compute Optimal Biosignal Transformers},
    author={Wagh, Neeraj and others},
    year={2024}
}

@inproceedings{misfitting,
    title={(Mis)fitting: A Survey of Scaling Laws},
    author={Moeini, Armin and others},
    booktitle={ICLR},
    year={2025}
}

@inproceedings{data_filtering_scaling,
    title={Scaling Laws for Data Filtering -- Data Curation Cannot Be Compute Agnostic},
    author={Goyal, Sachin and others},
    booktitle={CVPR},
    year={2024}
}

@article{chinchilla_robustness,
    title={Evaluating the Robustness of Chinchilla Compute-Optimal Scaling},
    author={others},
    journal={arXiv preprint arXiv:2509.23963},
    year={2025}
}

@article{kaplan_scaling,
    title={Scaling Laws for Neural Language Models},
    author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
    journal={arXiv preprint arXiv:2001.08361},
    year={2020}
}

@article{data_constrained,
    title={Scaling Data-Constrained Language Models},
    author={Muennighoff, Niklas and Rush, Alexander M. and Barak, Boaz and Le Scao, Teven and Piktus, Aleksandra and Tazi, Nouamane and Pyysalo, Sampo and Wolf, Thomas},
    journal={arXiv preprint arXiv:2305.16264},
    year={2023}
}

@article{mupt,
    title={{MuPT}: A Generative Symbolic Music Pretrained Transformer},
    author={Yang, Xinghua and others},
    journal={arXiv preprint arXiv:2404.06393},
    year={2024}
}

@article{precision_scaling,
    title={Scaling Laws for Precision},
    author={Kumar, Tanishq and others},
    journal={arXiv preprint arXiv:2411.04330},
    year={2024}
}

@article{reconciling_scaling,
    title={Reconciling {Kaplan} and {Chinchilla} Scaling Laws},
    author={Sardana, Nikhil and Frankle, Jonathan},
    journal={Transactions on Machine Learning Research},
    year={2024}
}

@article{quality_scaling,
    title={Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining},
    author={others},
    journal={arXiv preprint arXiv:2510.03313},
    year={2025}
}

@article{optimal_data_mixtures,
    title={Scaling Laws for Optimal Data Mixtures},
    author={others},
    journal={arXiv preprint arXiv:2507.09404},
    year={2025}
}

@article{redundancy_scaling,
    title={Scaling Laws are Redundancy Laws},
    author={others},
    journal={arXiv preprint arXiv:2509.20721},
    year={2025}
}

@article{moe_scaling,
    title={Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models},
    author={others},
    journal={arXiv preprint arXiv:2507.17702},
    year={2025}
}

@article{ai2_task_scaling,
    title={Establishing Task Scaling Laws via Compute-Efficient Model Ladders},
    author={others},
    journal={arXiv preprint arXiv:2412.04403},
    year={2024}
}

@article{farseer,
    title={Predictable Scale: {Part II}, {Farseer}: A Refined Scaling Law in Large Language Models},
    author={others},
    journal={arXiv preprint arXiv:2506.10972},
    year={2025}
}

@article{sld_agent,
    title={Can Language Models Discover Scaling Laws?},
    author={others},
    journal={arXiv preprint arXiv:2507.21184},
    year={2025}
}
